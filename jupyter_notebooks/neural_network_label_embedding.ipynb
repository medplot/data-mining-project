{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Artificial neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#imports\n",
    "from preprocessing.preprocessing import get_preprocessed_brfss_dataset\n",
    "from preprocessing.neural_network_preprocessing import get_number_of_numerical_features, NeuralNetworkPreprocessor, CATEGORICAL_COLUMNS, NUMERICAL_COLUMNS\n",
    "from visualization.general_plots import plot_class_frequencies\n",
    "from visualization.neural_network_plots import plot_loss, plot_accuracy\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch mps activated\n"
     ]
    }
   ],
   "source": [
    "use_mps = True\n",
    "use_cuda = False\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.has_mps and use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Torch mps activated\")\n",
    "\n",
    "if torch.has_cuda and use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Torch cuda activated\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the dataset it becomes clear that it is imbalanced"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         No  Yes\n147853  1.0  0.0\n257037  0.0  1.0\n217190  1.0  0.0\n166796  1.0  0.0\n227386  0.0  1.0\n43754   1.0  0.0\n148571  1.0  0.0\n83582   1.0  0.0\n23302   1.0  0.0\n211067  1.0  0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>No</th>\n      <th>Yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>147853</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>257037</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>217190</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>166796</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>227386</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>43754</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>148571</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>83582</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23302</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>211067</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = NeuralNetworkPreprocessor()\n",
    "data_train, data_validation, data_test, target_train, target_validation, target_test = preprocessor.get_preprocessed_dataset_for_neural_network()\n",
    "target_train.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 50\n",
      "Output size: 2\n",
      "Embedding sizes: [(6, 3), (2, 1), (3, 2), (6, 3), (4, 2), (3, 2), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (8, 4), (3, 2), (13, 7), (5, 3), (4, 2), (2, 1), (4, 2), (2, 1), (2, 1), (4, 2), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "#constants\n",
    "batch_size = 128\n",
    "output_size = target_train.shape[1]\n",
    "embedding_sizes = preprocessor.get_embedding_sizes()\n",
    "embedding_input_size = sum((nf for ni, nf in embedding_sizes))\n",
    "numerical_input_size = get_number_of_numerical_features()\n",
    "input_size = embedding_input_size + numerical_input_size\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"Embedding sizes: {embedding_sizes}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CustomBrfssDataset(data_utils.Dataset):\n",
    "    def __init__(self, dataset_complete, dataset_target):\n",
    "        self.data_categorical = dataset_complete[CATEGORICAL_COLUMNS]\n",
    "        self.data_categorical = torch.tensor(np.array(self.data_categorical)).int()\n",
    "\n",
    "        self.data_numerical = dataset_complete[NUMERICAL_COLUMNS]\n",
    "        self.data_numerical = torch.tensor(np.array(self.data_numerical)).float()\n",
    "\n",
    "        self.target = torch.tensor(np.array(dataset_target)).float()\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.data_categorical)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            return self.data_categorical[idx], self.data_numerical[idx], self.target[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create torch data loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_dataset = CustomBrfssDataset(data_train, target_train)\n",
    "validation_dataset = CustomBrfssDataset(data_validation, target_validation)\n",
    "\n",
    "# Custom weighted sampling required because otherwise the f2 score can be zero when all samples are from the same class, random sampling is not suitable\n",
    "class_count_train = [target_train[\"No\"].sum(),target_train[\"Yes\"].sum()]\n",
    "class_weights_train = 1./torch.tensor(class_count_train, dtype=torch.float)\n",
    "class_weights_train_all = class_weights_train[np.where(target_train.to_numpy()==1)[1]]\n",
    "\n",
    "class_count_validation = [target_validation[\"No\"].sum(),target_validation[\"Yes\"].sum()]\n",
    "class_weights_validation = 1./torch.tensor(class_count_validation, dtype=torch.float)\n",
    "class_weights_validation_all = class_weights_validation[np.where(target_validation.to_numpy()==1)[1]]\n",
    "\n",
    "weighted_sampler_train = data_utils.WeightedRandomSampler(weights=class_weights_train_all, num_samples=target_train.shape[0], replacement=True)\n",
    "weighted_sampler_validation = data_utils.WeightedRandomSampler(weights=class_weights_validation_all, num_samples=target_validation.shape[0], replacement=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=weighted_sampler_train)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, sampler=weighted_sampler_validation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create artificial net, define loss function and define optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "        self.batch_norm_numerical = nn.BatchNorm1d(numerical_input_size)\n",
    "        self.dropout_embedding = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.do1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(100, 5)\n",
    "        self.bn2 = nn.BatchNorm1d(5)\n",
    "        self.do2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(5, output_size)\n",
    "\n",
    "    def forward(self, input_categorical, input_numerical):\n",
    "        embedding_layers = []\n",
    "        for index,e in enumerate(self.embeddings):\n",
    "            embedding_layers.append(e(input_categorical[:,index]))\n",
    "        x_categorical = torch.cat(embedding_layers, 1)\n",
    "        x_categorical = self.dropout_embedding(x_categorical)\n",
    "\n",
    "        x_numerical = self.batch_norm_numerical(input_numerical)\n",
    "        x = torch.cat([x_categorical, x_numerical], 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu_(x)\n",
    "        x = self.do1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu_(x)\n",
    "        x = self.do2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_accuracy(model: Net, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            inputs_categorical, inputs_numerical, labels = data\n",
    "            inputs_categorical = inputs_categorical.to(device)\n",
    "            inputs_numerical = inputs_numerical.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs_categorical, inputs_numerical)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "def get_loss(model: Net, criterion, data_loader):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "\n",
    "        inputs_categorical, inputs_numerical, labels = data\n",
    "        inputs_categorical = inputs_categorical.to(device)\n",
    "        inputs_numerical = inputs_numerical.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs_categorical, inputs_numerical)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        pass\n",
    "\n",
    "    return running_loss/len(data_loader)\n",
    "\n",
    "def get_f_score(model: Net, data_loader):\n",
    "    running_predictions = []\n",
    "    running_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "\n",
    "        inputs_categorical, inputs_numerical, labels = data\n",
    "        inputs_categorical = inputs_categorical.to(device)\n",
    "        inputs_numerical = inputs_numerical.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs_categorical, inputs_numerical)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)\n",
    "        running_predictions.extend(predicted.detach().cpu().numpy().ravel())\n",
    "        running_labels.extend(labels.detach().cpu().numpy().ravel())\n",
    "\n",
    "        pass\n",
    "\n",
    "    return fbeta_score(running_labels, running_predictions, beta=2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train_network(model: Net, criterion, optimizer, data_train_loader, data_validation_loader, n_epochs=5):\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    loss_values_validation = []\n",
    "    accuracy_values_validation = []\n",
    "    f_scores = []\n",
    "    f_scores_validation = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, data in enumerate(data_train_loader, 0):\n",
    "\n",
    "            inputs_categorical, inputs_numerical, labels = data\n",
    "            inputs_categorical = inputs_categorical.to(device)\n",
    "            inputs_numerical = inputs_numerical.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs_categorical,inputs_numerical)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 batches\n",
    "                print(f'[{epoch + 1}, {(i + 1) * batch_size}] loss: {running_loss / i:.3f}')\n",
    "\n",
    "            pass\n",
    "\n",
    "        loss_values.append(running_loss/len(data_train_loader))\n",
    "        validation_loss = get_loss(model, criterion, data_validation_loader)\n",
    "        train_accuracy = 100 * get_accuracy(model, data_train_loader)\n",
    "        validation_accuracy = 100 * get_accuracy(model, data_validation_loader)\n",
    "        accuracy_values.append(train_accuracy)\n",
    "        accuracy_values_validation.append(validation_accuracy)\n",
    "        loss_values_validation.append(validation_loss)\n",
    "\n",
    "        f_train = get_f_score(net, data_train_loader)\n",
    "        f_validation = get_f_score(net, data_validation_loader)\n",
    "        f_scores.append(f_train)\n",
    "        f_scores_validation.append(f_validation)\n",
    "\n",
    "        print(f\"Epoch {epoch} loss: {str(running_loss/len(data_train_loader))}\")\n",
    "        print(f\"Epoch {epoch} validation loss: {validation_loss}\")\n",
    "        print(f'Train accuracy epoch {epoch}: {train_accuracy} %')\n",
    "        print(f'Validation accuracy epoch {epoch}: {validation_accuracy} %')\n",
    "        print(f\"Train F2-score : {f_train}\")\n",
    "        print(f\"Validation F2-score : {f_validation}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    return loss_values, loss_values_validation, accuracy_values, accuracy_values_validation, f_scores, f_scores_validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model without under or oversampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 12800] loss: 0.616\n",
      "[1, 25600] loss: 0.589\n",
      "[1, 38400] loss: 0.569\n",
      "[1, 51200] loss: 0.554\n",
      "[1, 64000] loss: 0.542\n",
      "[1, 76800] loss: 0.533\n",
      "[1, 89600] loss: 0.525\n",
      "[1, 102400] loss: 0.518\n",
      "[1, 115200] loss: 0.513\n",
      "[1, 128000] loss: 0.507\n",
      "[1, 140800] loss: 0.503\n",
      "[1, 153600] loss: 0.500\n",
      "[1, 166400] loss: 0.496\n",
      "Epoch 0 loss: 0.49544092969912473\n",
      "Epoch 0 validation loss: 0.4554537817158482\n",
      "Train accuracy epoch 0: 87.21178100685685 %\n",
      "Validation accuracy epoch 0: 87.21178100685685 %\n",
      "Train F2-score : 0.0\n",
      "Validation F2-score : 0.0\n",
      "[2, 12800] loss: 0.460\n",
      "[2, 25600] loss: 0.457\n",
      "[2, 38400] loss: 0.455\n",
      "[2, 51200] loss: 0.454\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [11], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m criterion_cross_entropy \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m      5\u001B[0m optimizer_adam \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(net\u001B[38;5;241m.\u001B[39mparameters(),lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.002\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m loss_values, loss_values_validation, accuracy_values, accuracy_values_validation, f_scores, f_scores_validation \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion_cross_entropy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_adam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_train_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_validation_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [10], line 23\u001B[0m, in \u001B[0;36mtrain_network\u001B[0;34m(model, criterion, optimizer, data_train_loader, data_validation_loader, n_epochs)\u001B[0m\n\u001B[1;32m     19\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 23\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs_categorical\u001B[49m\u001B[43m,\u001B[49m\u001B[43minputs_numerical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     25\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-SAPSE/Uni Mannheim/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn [8], line 19\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, input_categorical, input_numerical)\u001B[0m\n\u001B[1;32m     17\u001B[0m embedding_layers \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index,e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings):\n\u001B[0;32m---> 19\u001B[0m     embedding_layers\u001B[38;5;241m.\u001B[39mappend(\u001B[43me\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_categorical\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     20\u001B[0m x_categorical \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(embedding_layers, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     21\u001B[0m x_categorical \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout_embedding(x_categorical)\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-SAPSE/Uni Mannheim/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-SAPSE/Uni Mannheim/venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-SAPSE/Uni Mannheim/venv/lib/python3.8/site-packages/torch/nn/functional.py:2199\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2193\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[1;32m   2194\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[1;32m   2195\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m   2196\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[1;32m   2197\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[1;32m   2198\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[0;32m-> 2199\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "criterion_cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer_adam = optim.Adam(net.parameters(),lr=0.002)\n",
    "\n",
    "loss_values, loss_values_validation, accuracy_values, accuracy_values_validation, f_scores, f_scores_validation = train_network(model=net, criterion=criterion_cross_entropy, optimizer=optimizer_adam, data_train_loader=train_loader, data_validation_loader=validation_loader, n_epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_loss(loss_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_accuracy(accuracy_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model with undersampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = preprocessor.get_preprocessed_dataset_for_neural_network_undersampled()\n",
    "\n",
    "train_dataset = CustomBrfssDataset(data_train, target_train)\n",
    "test_dataset = CustomBrfssDataset(data_test, target_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "criterion_cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer_adam = optim.Adam(net.parameters(),lr=0.002)\n",
    "\n",
    "loss_values, accuracy_values = train_network(model=net, criterion=criterion_cross_entropy, optimizer=optimizer_adam, data_loader=train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}