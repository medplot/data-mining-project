{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# K-nearest neighbor\n",
    "In this notebook we want to train a K-nearest-neighbor classifier that should predict whether a patient has diabetes or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I516258/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from preprocessing.preprocessing_label_encoding import *\n",
    "from preprocessing.preprocessing_one_hot_encoding import *\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#used to store the results and compare at the end\n",
    "approach_list = []\n",
    "acc_list = []\n",
    "cr_list = []\n",
    "f2_list = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter tuning and preprocessing\n",
    "We will evaluate different parameters for the classifier (hyperparameter tuning) as well as different preprocessing steps based on the accuracy and F2-Score.\n",
    "\n",
    "In detail, we will vary regarding parameters:<br>\n",
    "\n",
    "| **Parameter** |                    **range Values**                    |\n",
    "|:-------------:|:------------------------------------------------------:|\n",
    "|    metric     | 'euclidean',<br>'cosine',<br>'manhattan',<br>'jaccard' |\n",
    "|  K-neighbors  |                          1-15                          |\n",
    "\n",
    "And we will vary for preprocessing:\n",
    "\n",
    "\n",
    "|              **Preprocessing**              |                           **Description**                           |\n",
    "|:-------------------------------------------:|:-------------------------------------------------------------------:|\n",
    "|               Label Encoding                |                           Label encoding                            |\n",
    "|     Label Encoding<br>+<br>Oversampling     |                   Label encoding and oversampling                   |\n",
    "|    Label Encoding<br>+<br>Undersampling     |                  Label encoding and undersampling                   |\n",
    "|            One Hot Encoding (1)             |           One hot encoding for all columns except yes/no            |\n",
    "|            One Hot Encoding (2)             |          One hot encoding for all columns including yes/no          |\n",
    "|  One Hot Encoding (1)<br>+<br>Oversampling  |   One hot encoding for all columns except yes/no and oversampling   |\n",
    "| One Hot Encoding (2) <br>+<br>Oversampling  | One hot encoding for all columns including yes/no and oversampling  |\n",
    "| One Hot Encoding (1)<br>+<br>Undersampling  |  One hot encoding for all columns except yes/no and undersampling   |\n",
    "| One Hot Encoding (2) <br>+<br>Undersampling | One hot encoding for all columns including yes/no and undersampling |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#metrics that should be used\n",
    "metrics = ('euclidean', 'cosine', 'manhattan', 'jaccard')\n",
    "n_neighbors_start = 1\n",
    "n_neighbors_end = 16"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The below defined function is used to store all the accuracy and F2-scores for a better comparison and evaluation capability at the end. It also returns the Accuracy score and F2-Score to see the performance directly under each method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def evaluation(target_validation, diabetes_test_prediction, k, metric, preprocessing):\n",
    "    approach = \"preprocessing: {} k= {} metric: {}\".format(preprocessing, k, metric)\n",
    "    approach_list.append(approach)\n",
    "    acc = accuracy_score(target_validation, diabetes_test_prediction)\n",
    "    acc_list.append(acc)\n",
    "    cr = classification_report(target_validation, diabetes_test_prediction)\n",
    "    cr_list.append(cr)\n",
    "    f2 = fbeta_score(target_validation, diabetes_test_prediction, beta=2)\n",
    "    f2_list.append(f2)\n",
    "    return \"{}\\n acc = {}\\n f2-score = {}\".format(approach, acc, f2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start with applying a K-nearest neighbor classifier to the train data and test against the validation data on how it performs by using the accuracy and F2-score.\n",
    "\n",
    "We will do so for each combination that is listed above by using two for loops. The following estimators are structured by the different style of preprocessing.\n",
    "\n",
    "At the end we test the best approach against the actual test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Label Encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: Label Encoding k= 1 metric: euclidean\n",
      " acc = 0.8216150921945501\n",
      " f2-score = 0.2991179498594841\n",
      "preprocessing: Label Encoding k= 1 metric: cosine\n",
      " acc = 0.8197854122997122\n",
      " f2-score = 0.30350420726306465\n",
      "preprocessing: Label Encoding k= 1 metric: manhattan\n",
      " acc = 0.8230717305574307\n",
      " f2-score = 0.30445551442963287\n",
      "preprocessing: Label Encoding k= 1 metric: jaccard\n",
      " acc = 0.8529328169964827\n",
      " f2-score = 0.0535538814281035\n",
      "preprocessing: Label Encoding k= 2 metric: euclidean\n",
      " acc = 0.8652431875510712\n",
      " f2-score = 0.13700404858299592\n",
      "preprocessing: Label Encoding k= 2 metric: cosine\n",
      " acc = 0.8652076597861229\n",
      " f2-score = 0.13989972505256346\n",
      "preprocessing: Label Encoding k= 2 metric: manhattan\n",
      " acc = 0.8662379649696238\n",
      " f2-score = 0.1413856768049752\n",
      "preprocessing: Label Encoding k= 2 metric: jaccard\n",
      " acc = 0.8721178100685686\n",
      " f2-score = 0.0\n",
      "preprocessing: Label Encoding k= 3 metric: euclidean\n",
      " acc = 0.8516005258109213\n",
      " f2-score = 0.2606394114304513\n",
      "preprocessing: Label Encoding k= 3 metric: cosine\n",
      " acc = 0.8518492201655594\n",
      " f2-score = 0.2670286720171853\n",
      "preprocessing: Label Encoding k= 3 metric: manhattan\n",
      " acc = 0.8518136924006111\n",
      " f2-score = 0.2600628460272333\n",
      "preprocessing: Label Encoding k= 3 metric: jaccard\n",
      " acc = 0.8716914768891889\n",
      " f2-score = 0.0008671522719389524\n",
      "preprocessing: Label Encoding k= 4 metric: euclidean\n",
      " acc = 0.868227519806729\n",
      " f2-score = 0.14938915713406137\n",
      "preprocessing: Label Encoding k= 4 metric: cosine\n",
      " acc = 0.8677478949799268\n",
      " f2-score = 0.15199534312140223\n",
      "preprocessing: Label Encoding k= 4 metric: manhattan\n",
      " acc = 0.8696308665221871\n",
      " f2-score = 0.15738341968911918\n",
      "preprocessing: Label Encoding k= 4 metric: jaccard\n",
      " acc = 0.8719046434788787\n",
      " f2-score = 0.0008675133597057394\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"Label Encoding\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_label_encoded_train_test_split()\n",
    "\n",
    "for n_neighbors in range(1,5):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Label Encoding + Oversampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: Label Encoding + Oversampling k= 5 metric: euclidean\n",
      " acc = 0.7431342594237397\n",
      " f2-score = 0.474826447395026\n",
      "preprocessing: Label Encoding + Oversampling k= 5 metric: cosine\n",
      " acc = 0.7372544143247949\n",
      " f2-score = 0.47358103893789544\n",
      "preprocessing: Label Encoding + Oversampling k= 5 metric: manhattan\n",
      " acc = 0.7453192169680606\n",
      " f2-score = 0.48150811126589665\n",
      "preprocessing: Label Encoding + Oversampling k= 5 metric: jaccard\n",
      " acc = 0.5244608661669095\n",
      " f2-score = 0.40419447092469013\n",
      "preprocessing: Label Encoding + Oversampling k= 6 metric: euclidean\n",
      " acc = 0.7478239243969161\n",
      " f2-score = 0.47096437722500617\n",
      "preprocessing: Label Encoding + Oversampling k= 6 metric: cosine\n",
      " acc = 0.7422815930649803\n",
      " f2-score = 0.4692105203950772\n",
      "preprocessing: Label Encoding + Oversampling k= 6 metric: manhattan\n",
      " acc = 0.750008881941237\n",
      " f2-score = 0.47706546889887025\n",
      "preprocessing: Label Encoding + Oversampling k= 6 metric: jaccard\n",
      " acc = 0.6273492734572068\n",
      " f2-score = 0.4067926606226278\n",
      "preprocessing: Label Encoding + Oversampling k= 7 metric: euclidean\n",
      " acc = 0.7175897964259068\n",
      " f2-score = 0.5034186031651366\n",
      "preprocessing: Label Encoding + Oversampling k= 7 metric: cosine\n",
      " acc = 0.7100756741393399\n",
      " f2-score = 0.5058271625715247\n",
      "preprocessing: Label Encoding + Oversampling k= 7 metric: manhattan\n",
      " acc = 0.7180161296052865\n",
      " f2-score = 0.5063102560282631\n",
      "preprocessing: Label Encoding + Oversampling k= 7 metric: jaccard\n",
      " acc = 0.5255266991153587\n",
      " f2-score = 0.4039606366176649\n",
      "preprocessing: Label Encoding + Oversampling k= 8 metric: euclidean\n",
      " acc = 0.7277152094361744\n",
      " f2-score = 0.4976030888700896\n",
      "preprocessing: Label Encoding + Oversampling k= 8 metric: cosine\n",
      " acc = 0.7206451842114613\n",
      " f2-score = 0.49962482581198414\n",
      "preprocessing: Label Encoding + Oversampling k= 8 metric: manhattan\n",
      " acc = 0.7275198067289587\n",
      " f2-score = 0.4986996098829649\n",
      "preprocessing: Label Encoding + Oversampling k= 8 metric: jaccard\n",
      " acc = 0.6518279035065904\n",
      " f2-score = 0.4082295834433454\n",
      "preprocessing: Label Encoding + Oversampling k= 9 metric: euclidean\n",
      " acc = 0.7005719970156677\n",
      " f2-score = 0.5176822352819879\n",
      "preprocessing: Label Encoding + Oversampling k= 9 metric: cosine\n",
      " acc = 0.6928624720218851\n",
      " f2-score = 0.5161718192021223\n",
      "preprocessing: Label Encoding + Oversampling k= 9 metric: manhattan\n",
      " acc = 0.701531246669272\n",
      " f2-score = 0.5186683453385895\n",
      "preprocessing: Label Encoding + Oversampling k= 9 metric: jaccard\n",
      " acc = 0.651668028564323\n",
      " f2-score = 0.4082399285250163\n",
      "preprocessing: Label Encoding + Oversampling k= 10 metric: euclidean\n",
      " acc = 0.7188687959640458\n",
      " f2-score = 0.5097959965113064\n",
      "preprocessing: Label Encoding + Oversampling k= 10 metric: cosine\n",
      " acc = 0.711372437559953\n",
      " f2-score = 0.5077804215535575\n",
      "preprocessing: Label Encoding + Oversampling k= 10 metric: manhattan\n",
      " acc = 0.7203254343269265\n",
      " f2-score = 0.5131424922847718\n",
      "preprocessing: Label Encoding + Oversampling k= 10 metric: jaccard\n",
      " acc = 0.7276263900238036\n",
      " f2-score = 0.3014716388055436\n",
      "preprocessing: Label Encoding + Oversampling k= 11 metric: euclidean\n",
      " acc = 0.697250150993001\n",
      " f2-score = 0.5274442538593481\n",
      "preprocessing: Label Encoding + Oversampling k= 11 metric: cosine\n",
      " acc = 0.6890965289373645\n",
      " f2-score = 0.5237459816825378\n",
      "preprocessing: Label Encoding + Oversampling k= 11 metric: manhattan\n",
      " acc = 0.6985469144136143\n",
      " f2-score = 0.5287960255964671\n",
      "preprocessing: Label Encoding + Oversampling k= 11 metric: jaccard\n",
      " acc = 0.7263651543681388\n",
      " f2-score = 0.3013809331400185\n",
      "preprocessing: Label Encoding + Oversampling k= 12 metric: euclidean\n",
      " acc = 0.7165417273599318\n",
      " f2-score = 0.5198051126297141\n",
      "preprocessing: Label Encoding + Oversampling k= 12 metric: cosine\n",
      " acc = 0.7099513269620208\n",
      " f2-score = 0.5190857238367841\n",
      "preprocessing: Label Encoding + Oversampling k= 12 metric: manhattan\n",
      " acc = 0.7176253241908551\n",
      " f2-score = 0.521226962781566\n",
      "preprocessing: Label Encoding + Oversampling k= 12 metric: jaccard\n",
      " acc = 0.7275908622588553\n",
      " f2-score = 0.301457281645871\n",
      "preprocessing: Label Encoding + Oversampling k= 13 metric: euclidean\n",
      " acc = 0.698635733825985\n",
      " f2-score = 0.5331932430226864\n",
      "preprocessing: Label Encoding + Oversampling k= 13 metric: cosine\n",
      " acc = 0.6902867090631328\n",
      " f2-score = 0.5303580800129184\n",
      "preprocessing: Label Encoding + Oversampling k= 13 metric: manhattan\n",
      " acc = 0.6989377198280456\n",
      " f2-score = 0.5355431879803478\n",
      "preprocessing: Label Encoding + Oversampling k= 13 metric: jaccard\n",
      " acc = 0.7191352542011582\n",
      " f2-score = 0.30582797616254515\n",
      "preprocessing: Label Encoding + Oversampling k= 14 metric: euclidean\n",
      " acc = 0.7194372402032189\n",
      " f2-score = 0.5269434535745193\n",
      "preprocessing: Label Encoding + Oversampling k= 14 metric: cosine\n",
      " acc = 0.7119586456816002\n",
      " f2-score = 0.5256892230576442\n",
      "preprocessing: Label Encoding + Oversampling k= 14 metric: manhattan\n",
      " acc = 0.7200412122073401\n",
      " f2-score = 0.529985023308794\n",
      "preprocessing: Label Encoding + Oversampling k= 14 metric: jaccard\n",
      " acc = 0.7285501119124596\n",
      " f2-score = 0.3013211866832014\n",
      "preprocessing: Label Encoding + Oversampling k= 15 metric: euclidean\n",
      " acc = 0.702454968557928\n",
      " f2-score = 0.5368296029324432\n",
      "preprocessing: Label Encoding + Oversampling k= 15 metric: cosine\n",
      " acc = 0.6946566241517746\n",
      " f2-score = 0.5351570415400202\n",
      "preprocessing: Label Encoding + Oversampling k= 15 metric: manhattan\n",
      " acc = 0.7035208015063772\n",
      " f2-score = 0.5397407278461569\n",
      "preprocessing: Label Encoding + Oversampling k= 15 metric: jaccard\n",
      " acc = 0.7274665150815363\n",
      " f2-score = 0.3014070423541176\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"Label Encoding + Oversampling\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_label_encoded_train_test_split_oversampled()\n",
    "\n",
    "for n_neighbors in range(5,16):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Label Encoding + Undersampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: Label Encoding + Undersampling k= 1 metric: euclidean\n",
      " acc = 0.6729136320034107\n",
      " f2-score = 0.48936640668971626\n",
      "preprocessing: Label Encoding + Undersampling k= 1 metric: cosine\n",
      " acc = 0.6658258428962234\n",
      " f2-score = 0.4914007356596083\n",
      "preprocessing: Label Encoding + Undersampling k= 1 metric: manhattan\n",
      " acc = 0.6762354780260774\n",
      " f2-score = 0.4929946577965931\n",
      "preprocessing: Label Encoding + Undersampling k= 1 metric: jaccard\n",
      " acc = 0.8720289906561978\n",
      " f2-score = 0.000520706772659423\n",
      "preprocessing: Label Encoding + Undersampling k= 2 metric: euclidean\n",
      " acc = 0.7795324546132802\n",
      " f2-score = 0.4364077669902912\n",
      "preprocessing: Label Encoding + Undersampling k= 2 metric: cosine\n",
      " acc = 0.7737769566916546\n",
      " f2-score = 0.4364812232533538\n",
      "preprocessing: Label Encoding + Undersampling k= 2 metric: manhattan\n",
      " acc = 0.7817351760400754\n",
      " f2-score = 0.43892944038929443\n",
      "preprocessing: Label Encoding + Undersampling k= 2 metric: jaccard\n",
      " acc = 0.8720289906561978\n",
      " f2-score = 0.000520706772659423\n",
      "preprocessing: Label Encoding + Undersampling k= 3 metric: euclidean\n",
      " acc = 0.695047429566206\n",
      " f2-score = 0.5321244365024571\n",
      "preprocessing: Label Encoding + Undersampling k= 3 metric: cosine\n",
      " acc = 0.6847799054961452\n",
      " f2-score = 0.5311674273360557\n",
      "preprocessing: Label Encoding + Undersampling k= 3 metric: manhattan\n",
      " acc = 0.6979607062919672\n",
      " f2-score = 0.5385381519573967\n",
      "preprocessing: Label Encoding + Undersampling k= 3 metric: jaccard\n",
      " acc = 0.8719934628912495\n",
      " f2-score = 0.0005206706237634072\n",
      "preprocessing: Label Encoding + Undersampling k= 4 metric: euclidean\n",
      " acc = 0.7597612534195474\n",
      " f2-score = 0.5059841482524471\n",
      "preprocessing: Label Encoding + Undersampling k= 4 metric: cosine\n",
      " acc = 0.7514655203041176\n",
      " f2-score = 0.5082829639579135\n",
      "preprocessing: Label Encoding + Undersampling k= 4 metric: manhattan\n",
      " acc = 0.7603652254236686\n",
      " f2-score = 0.51026426222071\n",
      "preprocessing: Label Encoding + Undersampling k= 4 metric: jaccard\n",
      " acc = 0.6606387892137706\n",
      " f2-score = 0.37763594268193484\n",
      "preprocessing: Label Encoding + Undersampling k= 5 metric: euclidean\n",
      " acc = 0.70314775997442\n",
      " f2-score = 0.5517921803165303\n",
      "preprocessing: Label Encoding + Undersampling k= 5 metric: cosine\n",
      " acc = 0.6925604860198245\n",
      " f2-score = 0.551994872616568\n",
      "preprocessing: Label Encoding + Undersampling k= 5 metric: manhattan\n",
      " acc = 0.7047998010445163\n",
      " f2-score = 0.556640426354224\n",
      "preprocessing: Label Encoding + Undersampling k= 5 metric: jaccard\n",
      " acc = 0.5655309624471524\n",
      " f2-score = 0.3947633065769505\n",
      "preprocessing: Label Encoding + Undersampling k= 6 metric: euclidean\n",
      " acc = 0.7496003126443316\n",
      " f2-score = 0.5379405879636755\n",
      "preprocessing: Label Encoding + Undersampling k= 6 metric: cosine\n",
      " acc = 0.7408427185845738\n",
      " f2-score = 0.5378909469483365\n",
      "preprocessing: Label Encoding + Undersampling k= 6 metric: manhattan\n",
      " acc = 0.7489075212278395\n",
      " f2-score = 0.5386000351308624\n",
      "preprocessing: Label Encoding + Undersampling k= 6 metric: jaccard\n",
      " acc = 0.5680711976409564\n",
      " f2-score = 0.3916238060249817\n",
      "preprocessing: Label Encoding + Undersampling k= 7 metric: euclidean\n",
      " acc = 0.7065406615269834\n",
      " f2-score = 0.5631212016504401\n",
      "preprocessing: Label Encoding + Undersampling k= 7 metric: cosine\n",
      " acc = 0.6959178598074395\n",
      " f2-score = 0.561807877926187\n",
      "preprocessing: Label Encoding + Undersampling k= 7 metric: manhattan\n",
      " acc = 0.7093473549578996\n",
      " f2-score = 0.5686374274956751\n",
      "preprocessing: Label Encoding + Undersampling k= 7 metric: jaccard\n",
      " acc = 0.2542899776175081\n",
      " f2-score = 0.44640549899219595\n",
      "preprocessing: Label Encoding + Undersampling k= 8 metric: euclidean\n",
      " acc = 0.74155327388354\n",
      " f2-score = 0.5531497759393312\n",
      "preprocessing: Label Encoding + Undersampling k= 8 metric: cosine\n",
      " acc = 0.7343766653639819\n",
      " f2-score = 0.5536147914625315\n",
      "preprocessing: Label Encoding + Undersampling k= 8 metric: manhattan\n",
      " acc = 0.7435783564855935\n",
      " f2-score = 0.5581184789169613\n",
      "preprocessing: Label Encoding + Undersampling k= 8 metric: jaccard\n",
      " acc = 0.5645894766760223\n",
      " f2-score = 0.39599532727339093\n",
      "preprocessing: Label Encoding + Undersampling k= 9 metric: euclidean\n",
      " acc = 0.7089032578960458\n",
      " f2-score = 0.57258260940164\n",
      "preprocessing: Label Encoding + Undersampling k= 9 metric: cosine\n",
      " acc = 0.6992574697125804\n",
      " f2-score = 0.5714514546583104\n",
      "preprocessing: Label Encoding + Undersampling k= 9 metric: manhattan\n",
      " acc = 0.7114079653249014\n",
      " f2-score = 0.5754795663052543\n",
      "preprocessing: Label Encoding + Undersampling k= 9 metric: jaccard\n",
      " acc = 0.2559953103350268\n",
      " f2-score = 0.4466411344581306\n",
      "preprocessing: Label Encoding + Undersampling k= 10 metric: euclidean\n",
      " acc = 0.7375741642093296\n",
      " f2-score = 0.5621270247229327\n",
      "preprocessing: Label Encoding + Undersampling k= 10 metric: cosine\n",
      " acc = 0.7305041389846165\n",
      " f2-score = 0.5660635388175889\n",
      "preprocessing: Label Encoding + Undersampling k= 10 metric: manhattan\n",
      " acc = 0.7407183714072547\n",
      " f2-score = 0.5693771737043123\n",
      "preprocessing: Label Encoding + Undersampling k= 10 metric: jaccard\n",
      " acc = 0.34177709880271434\n",
      " f2-score = 0.4496385272607778\n",
      "preprocessing: Label Encoding + Undersampling k= 11 metric: euclidean\n",
      " acc = 0.7096138131950119\n",
      " f2-score = 0.5744559922052616\n",
      "preprocessing: Label Encoding + Undersampling k= 11 metric: cosine\n",
      " acc = 0.6992041780651579\n",
      " f2-score = 0.5751169018024859\n",
      "preprocessing: Label Encoding + Undersampling k= 11 metric: manhattan\n",
      " acc = 0.712402742743454\n",
      " f2-score = 0.5798117771408826\n",
      "preprocessing: Label Encoding + Undersampling k= 11 metric: jaccard\n",
      " acc = 0.2549827690340001\n",
      " f2-score = 0.4466305570278062\n",
      "preprocessing: Label Encoding + Undersampling k= 12 metric: euclidean\n",
      " acc = 0.7352470956052155\n",
      " f2-score = 0.5694247572302029\n",
      "preprocessing: Label Encoding + Undersampling k= 12 metric: cosine\n",
      " acc = 0.7262940988382421\n",
      " f2-score = 0.5712498696966538\n",
      "preprocessing: Label Encoding + Undersampling k= 12 metric: manhattan\n",
      " acc = 0.7371478310299499\n",
      " f2-score = 0.5743915343915343\n",
      "preprocessing: Label Encoding + Undersampling k= 12 metric: jaccard\n",
      " acc = 0.3319891995594557\n",
      " f2-score = 0.38636753113218325\n",
      "preprocessing: Label Encoding + Undersampling k= 13 metric: euclidean\n",
      " acc = 0.7113191459125306\n",
      " f2-score = 0.5797174980720055\n",
      "preprocessing: Label Encoding + Undersampling k= 13 metric: cosine\n",
      " acc = 0.7002167193661847\n",
      " f2-score = 0.5785404638711868\n",
      "preprocessing: Label Encoding + Undersampling k= 13 metric: manhattan\n",
      " acc = 0.7133619923970583\n",
      " f2-score = 0.5847359551702435\n",
      "preprocessing: Label Encoding + Undersampling k= 13 metric: jaccard\n",
      " acc = 0.25587096315770774\n",
      " f2-score = 0.4467069431687344\n",
      "preprocessing: Label Encoding + Undersampling k= 14 metric: euclidean\n",
      " acc = 0.7330266102959463\n",
      " f2-score = 0.5751524069791886\n",
      "preprocessing: Label Encoding + Undersampling k= 14 metric: cosine\n",
      " acc = 0.7237538636444382\n",
      " f2-score = 0.577066423720391\n",
      "preprocessing: Label Encoding + Undersampling k= 14 metric: manhattan\n",
      " acc = 0.7353359150175862\n",
      " f2-score = 0.5809355544337876\n",
      "preprocessing: Label Encoding + Undersampling k= 14 metric: jaccard\n",
      " acc = 0.25711443493089847\n",
      " f2-score = 0.4465268693253202\n",
      "preprocessing: Label Encoding + Undersampling k= 15 metric: euclidean\n",
      " acc = 0.7118698262692295\n",
      " f2-score = 0.5808619044719161\n",
      "preprocessing: Label Encoding + Undersampling k= 15 metric: cosine\n",
      " acc = 0.7016911216115395\n",
      " f2-score = 0.5828006392329204\n",
      "preprocessing: Label Encoding + Undersampling k= 15 metric: manhattan\n",
      " acc = 0.7127402565104629\n",
      " f2-score = 0.5862621103409137\n",
      "preprocessing: Label Encoding + Undersampling k= 15 metric: jaccard\n",
      " acc = 0.2568479766937862\n",
      " f2-score = 0.4471844660194176\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"Label Encoding + Undersampling\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_label_encoded_train_test_split_undersampled()\n",
    "\n",
    "for n_neighbors in range(n_neighbors_start, n_neighbors_end):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding (1)\n",
    "yes/no values not one hot encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: One Hot Encoding (1) k= 1 metric: euclidean\n",
      " acc = 0.8200518705368245\n",
      " f2-score = 0.3002190975400061\n",
      "preprocessing: One Hot Encoding (1) k= 1 metric: cosine\n",
      " acc = 0.8217039116069208\n",
      " f2-score = 0.2938797143494756\n",
      "preprocessing: One Hot Encoding (1) k= 1 metric: manhattan\n",
      " acc = 0.8205847870110491\n",
      " f2-score = 0.30083534537784806\n",
      "preprocessing: One Hot Encoding (1) k= 1 metric: jaccard\n",
      " acc = 0.8191459125306427\n",
      " f2-score = 0.30210811708072816\n",
      "preprocessing: One Hot Encoding (1) k= 2 metric: euclidean\n",
      " acc = 0.8653852986108644\n",
      " f2-score = 0.13673330525224378\n",
      "preprocessing: One Hot Encoding (1) k= 2 metric: cosine\n",
      " acc = 0.8655096457881835\n",
      " f2-score = 0.13139089196387968\n",
      "preprocessing: One Hot Encoding (1) k= 2 metric: manhattan\n",
      " acc = 0.8649944931964331\n",
      " f2-score = 0.13724792024083127\n",
      "preprocessing: One Hot Encoding (1) k= 2 metric: jaccard\n",
      " acc = 0.8639819518954063\n",
      " f2-score = 0.13836863859467835\n",
      "preprocessing: One Hot Encoding (1) k= 3 metric: euclidean\n",
      " acc = 0.8506235122748428\n",
      " f2-score = 0.26408345752608053\n",
      "preprocessing: One Hot Encoding (1) k= 3 metric: cosine\n",
      " acc = 0.8516182896933954\n",
      " f2-score = 0.2505557892206934\n",
      "preprocessing: One Hot Encoding (1) k= 3 metric: manhattan\n",
      " acc = 0.8509610260418518\n",
      " f2-score = 0.2655658549670651\n",
      "preprocessing: One Hot Encoding (1) k= 3 metric: jaccard\n",
      " acc = 0.8496997903861868\n",
      " f2-score = 0.26420708122582565\n",
      "preprocessing: One Hot Encoding (1) k= 4 metric: euclidean\n",
      " acc = 0.8684584502788929\n",
      " f2-score = 0.15613887631732076\n",
      "preprocessing: One Hot Encoding (1) k= 4 metric: cosine\n",
      " acc = 0.8680498809819874\n",
      " f2-score = 0.14154776221276041\n",
      "preprocessing: One Hot Encoding (1) k= 4 metric: manhattan\n",
      " acc = 0.8679077699221942\n",
      " f2-score = 0.15021528699537054\n",
      "preprocessing: One Hot Encoding (1) k= 4 metric: jaccard\n",
      " acc = 0.866149145557253\n",
      " f2-score = 0.1550325198016614\n",
      "preprocessing: One Hot Encoding (1) k= 5 metric: euclidean\n",
      " acc = 0.8612285501119125\n",
      " f2-score = 0.23895193283513239\n",
      "preprocessing: One Hot Encoding (1) k= 5 metric: cosine\n",
      " acc = 0.8612996056418091\n",
      " f2-score = 0.22244942218917496\n",
      "preprocessing: One Hot Encoding (1) k= 5 metric: manhattan\n",
      " acc = 0.8605712864603687\n",
      " f2-score = 0.23227157907707474\n",
      "preprocessing: One Hot Encoding (1) k= 5 metric: jaccard\n",
      " acc = 0.85886595374285\n",
      " f2-score = 0.2421277896667686\n",
      "preprocessing: One Hot Encoding (1) k= 6 metric: euclidean\n",
      " acc = 0.8704835328809465\n",
      " f2-score = 0.15549928580703803\n",
      "preprocessing: One Hot Encoding (1) k= 6 metric: cosine\n",
      " acc = 0.8706078800582655\n",
      " f2-score = 0.14452092780478254\n",
      "preprocessing: One Hot Encoding (1) k= 6 metric: manhattan\n",
      " acc = 0.8704480051159982\n",
      " f2-score = 0.1541184877224893\n",
      "preprocessing: One Hot Encoding (1) k= 6 metric: jaccard\n",
      " acc = 0.8680854087469357\n",
      " f2-score = 0.15239266185653735\n",
      "preprocessing: One Hot Encoding (1) k= 7 metric: euclidean\n",
      " acc = 0.8670373396809606\n",
      " f2-score = 0.22127499529868988\n",
      "preprocessing: One Hot Encoding (1) k= 7 metric: cosine\n",
      " acc = 0.8674636728603403\n",
      " f2-score = 0.2066233110241192\n",
      "preprocessing: One Hot Encoding (1) k= 7 metric: manhattan\n",
      " acc = 0.8667531175613742\n",
      " f2-score = 0.21987838515546637\n",
      "preprocessing: One Hot Encoding (1) k= 7 metric: jaccard\n",
      " acc = 0.8642484101325185\n",
      " f2-score = 0.22359938961726514\n",
      "preprocessing: One Hot Encoding (1) k= 8 metric: euclidean\n",
      " acc = 0.8718158240665079\n",
      " f2-score = 0.1508332518018459\n",
      "preprocessing: One Hot Encoding (1) k= 8 metric: cosine\n",
      " acc = 0.8714427825345508\n",
      " f2-score = 0.13779656573600735\n",
      "preprocessing: One Hot Encoding (1) k= 8 metric: manhattan\n",
      " acc = 0.8720645184211461\n",
      " f2-score = 0.15304625615281806\n",
      "preprocessing: One Hot Encoding (1) k= 8 metric: jaccard\n",
      " acc = 0.869844033111877\n",
      " f2-score = 0.15577384425857485\n",
      "preprocessing: One Hot Encoding (1) k= 9 metric: euclidean\n",
      " acc = 0.8695953387572388\n",
      " f2-score = 0.2033333333333333\n",
      "preprocessing: One Hot Encoding (1) k= 9 metric: cosine\n",
      " acc = 0.869737449817032\n",
      " f2-score = 0.18796752125823157\n",
      "preprocessing: One Hot Encoding (1) k= 9 metric: manhattan\n",
      " acc = 0.8703769495861016\n",
      " f2-score = 0.2076986544808327\n",
      "preprocessing: One Hot Encoding (1) k= 9 metric: jaccard\n",
      " acc = 0.867072867445909\n",
      " f2-score = 0.21239773442416615\n",
      "preprocessing: One Hot Encoding (1) k= 10 metric: euclidean\n",
      " acc = 0.8725974348953708\n",
      " f2-score = 0.15197022624138942\n",
      "preprocessing: One Hot Encoding (1) k= 10 metric: cosine\n",
      " acc = 0.8724197960706292\n",
      " f2-score = 0.13664881910455606\n",
      "preprocessing: One Hot Encoding (1) k= 10 metric: manhattan\n",
      " acc = 0.8727928376025864\n",
      " f2-score = 0.15079728140112403\n",
      "preprocessing: One Hot Encoding (1) k= 10 metric: jaccard\n",
      " acc = 0.8710164493551711\n",
      " f2-score = 0.15565087411451226\n",
      "preprocessing: One Hot Encoding (1) k= 11 metric: euclidean\n",
      " acc = 0.8705723522933172\n",
      " f2-score = 0.19430341348149568\n",
      "preprocessing: One Hot Encoding (1) k= 11 metric: cosine\n",
      " acc = 0.8707499911180587\n",
      " f2-score = 0.1789446606000193\n",
      "preprocessing: One Hot Encoding (1) k= 11 metric: manhattan\n",
      " acc = 0.8710164493551711\n",
      " f2-score = 0.1950479233226837\n",
      "preprocessing: One Hot Encoding (1) k= 11 metric: jaccard\n",
      " acc = 0.8683873947489963\n",
      " f2-score = 0.201730476673428\n",
      "preprocessing: One Hot Encoding (1) k= 12 metric: euclidean\n",
      " acc = 0.8731481152520695\n",
      " f2-score = 0.1510494997711371\n",
      "preprocessing: One Hot Encoding (1) k= 12 metric: cosine\n",
      " acc = 0.8721888655984652\n",
      " f2-score = 0.1275543836519446\n",
      "preprocessing: One Hot Encoding (1) k= 12 metric: manhattan\n",
      " acc = 0.8727750737201123\n",
      " f2-score = 0.14648597335428326\n",
      "preprocessing: One Hot Encoding (1) k= 12 metric: jaccard\n",
      " acc = 0.870661171705688\n",
      " f2-score = 0.14867769671704012\n",
      "preprocessing: One Hot Encoding (1) k= 13 metric: euclidean\n",
      " acc = 0.8720822823036203\n",
      " f2-score = 0.18906079476150736\n",
      "preprocessing: One Hot Encoding (1) k= 13 metric: cosine\n",
      " acc = 0.8713717270046542\n",
      " f2-score = 0.16244082744309707\n",
      "preprocessing: One Hot Encoding (1) k= 13 metric: manhattan\n",
      " acc = 0.8719046434788787\n",
      " f2-score = 0.1858813700918964\n",
      "preprocessing: One Hot Encoding (1) k= 13 metric: jaccard\n",
      " acc = 0.8697552136995061\n",
      " f2-score = 0.1901887515569608\n",
      "preprocessing: One Hot Encoding (1) k= 14 metric: euclidean\n",
      " acc = 0.8737520872561907\n",
      " f2-score = 0.14890898368390013\n",
      "preprocessing: One Hot Encoding (1) k= 14 metric: cosine\n",
      " acc = 0.8730770597221729\n",
      " f2-score = 0.1238425925925926\n",
      "preprocessing: One Hot Encoding (1) k= 14 metric: manhattan\n",
      " acc = 0.8734678651366042\n",
      " f2-score = 0.1451295506723516\n",
      "preprocessing: One Hot Encoding (1) k= 14 metric: jaccard\n",
      " acc = 0.8712829075922833\n",
      " f2-score = 0.14423862262733184\n",
      "preprocessing: One Hot Encoding (1) k= 15 metric: euclidean\n",
      " acc = 0.8730770597221729\n",
      " f2-score = 0.1798514691637068\n",
      "preprocessing: One Hot Encoding (1) k= 15 metric: cosine\n",
      " acc = 0.873325754076811\n",
      " f2-score = 0.15997654111820672\n",
      "preprocessing: One Hot Encoding (1) k= 15 metric: manhattan\n",
      " acc = 0.8730948236046471\n",
      " f2-score = 0.18135467423924617\n",
      "preprocessing: One Hot Encoding (1) k= 15 metric: jaccard\n",
      " acc = 0.8700216719366185\n",
      " f2-score = 0.18347006794000773\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"One Hot Encoding (1)\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_one_hot_encoded_train_test_split()\n",
    "\n",
    "for n_neighbors in range(n_neighbors_start, n_neighbors_end):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding (2)\n",
    "all columns one hot encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: One Hot Encoding (2) k= 1 metric: euclidean\n",
      " acc = 0.8222190641986713\n",
      " f2-score = 0.3062442735374962\n",
      "preprocessing: One Hot Encoding (2) k= 1 metric: cosine\n",
      " acc = 0.8196077734749707\n",
      " f2-score = 0.30342022248049144\n",
      "preprocessing: One Hot Encoding (2) k= 1 metric: manhattan\n",
      " acc = 0.8229651472625857\n",
      " f2-score = 0.30696756621550264\n",
      "preprocessing: One Hot Encoding (2) k= 1 metric: jaccard\n",
      " acc = 0.8211177034852737\n",
      " f2-score = 0.30364821750589543\n",
      "preprocessing: One Hot Encoding (2) k= 2 metric: euclidean\n",
      " acc = 0.8656517568479767\n",
      " f2-score = 0.13863695257838818\n",
      "preprocessing: One Hot Encoding (2) k= 2 metric: cosine\n",
      " acc = 0.8644615767222084\n",
      " f2-score = 0.13604451777799348\n",
      "preprocessing: One Hot Encoding (2) k= 2 metric: manhattan\n",
      " acc = 0.8656517568479767\n",
      " f2-score = 0.13817793040886414\n",
      "preprocessing: One Hot Encoding (2) k= 2 metric: jaccard\n",
      " acc = 0.8652076597861229\n",
      " f2-score = 0.1354635901092135\n",
      "preprocessing: One Hot Encoding (2) k= 3 metric: euclidean\n",
      " acc = 0.8516182896933954\n",
      " f2-score = 0.2589045196049087\n",
      "preprocessing: One Hot Encoding (2) k= 3 metric: cosine\n",
      " acc = 0.8509432621593775\n",
      " f2-score = 0.2604851236706895\n",
      "preprocessing: One Hot Encoding (2) k= 3 metric: manhattan\n",
      " acc = 0.8519380395779301\n",
      " f2-score = 0.2575666766556788\n",
      "preprocessing: One Hot Encoding (2) k= 3 metric: jaccard\n",
      " acc = 0.8515827619284471\n",
      " f2-score = 0.26223776223776224\n",
      "preprocessing: One Hot Encoding (2) k= 4 metric: euclidean\n",
      " acc = 0.8679788254520908\n",
      " f2-score = 0.14840737500405043\n",
      "preprocessing: One Hot Encoding (2) k= 4 metric: cosine\n",
      " acc = 0.8682808114541514\n",
      " f2-score = 0.15335835382425259\n",
      "preprocessing: One Hot Encoding (2) k= 4 metric: manhattan\n",
      " acc = 0.8682452836892031\n",
      " f2-score = 0.14771695420936568\n",
      "preprocessing: One Hot Encoding (2) k= 4 metric: jaccard\n",
      " acc = 0.8685295058087895\n",
      " f2-score = 0.153731633115412\n",
      "preprocessing: One Hot Encoding (2) k= 5 metric: euclidean\n",
      " acc = 0.8621345081180943\n",
      " f2-score = 0.2341627562361077\n",
      "preprocessing: One Hot Encoding (2) k= 5 metric: cosine\n",
      " acc = 0.862791771769638\n",
      " f2-score = 0.2440525651678823\n",
      "preprocessing: One Hot Encoding (2) k= 5 metric: manhattan\n",
      " acc = 0.862098980353146\n",
      " f2-score = 0.2313469740990295\n",
      "preprocessing: One Hot Encoding (2) k= 5 metric: jaccard\n",
      " acc = 0.8607666891675845\n",
      " f2-score = 0.23262979510090895\n",
      "preprocessing: One Hot Encoding (2) k= 6 metric: euclidean\n",
      " acc = 0.8700571997015668\n",
      " f2-score = 0.14988945246455976\n",
      "preprocessing: One Hot Encoding (2) k= 6 metric: cosine\n",
      " acc = 0.8706256439407397\n",
      " f2-score = 0.15432098765432098\n",
      "preprocessing: One Hot Encoding (2) k= 6 metric: manhattan\n",
      " acc = 0.87019931076136\n",
      " f2-score = 0.14839885446498308\n",
      "preprocessing: One Hot Encoding (2) k= 6 metric: jaccard\n",
      " acc = 0.8694709915799197\n",
      " f2-score = 0.148659626320065\n",
      "preprocessing: One Hot Encoding (2) k= 7 metric: euclidean\n",
      " acc = 0.867428145095392\n",
      " f2-score = 0.21526511101327128\n",
      "preprocessing: One Hot Encoding (2) k= 7 metric: cosine\n",
      " acc = 0.8674992006252886\n",
      " f2-score = 0.22359761830147287\n",
      "preprocessing: One Hot Encoding (2) k= 7 metric: manhattan\n",
      " acc = 0.8674814367428145\n",
      " f2-score = 0.21686519785020586\n",
      "preprocessing: One Hot Encoding (2) k= 7 metric: jaccard\n",
      " acc = 0.8662202010871496\n",
      " f2-score = 0.21408839779005526\n",
      "preprocessing: One Hot Encoding (2) k= 8 metric: euclidean\n",
      " acc = 0.8726329626603191\n",
      " f2-score = 0.15059945771128022\n",
      "preprocessing: One Hot Encoding (2) k= 8 metric: cosine\n",
      " acc = 0.8722421572458876\n",
      " f2-score = 0.15233062595818248\n",
      "preprocessing: One Hot Encoding (2) k= 8 metric: manhattan\n",
      " acc = 0.8721355739510427\n",
      " f2-score = 0.14800535825138042\n",
      "preprocessing: One Hot Encoding (2) k= 8 metric: jaccard\n",
      " acc = 0.870785518883007\n",
      " f2-score = 0.14855839713308358\n",
      "preprocessing: One Hot Encoding (2) k= 9 metric: euclidean\n",
      " acc = 0.8706078800582655\n",
      " f2-score = 0.20530748450659464\n",
      "preprocessing: One Hot Encoding (2) k= 9 metric: cosine\n",
      " acc = 0.8695420471098163\n",
      " f2-score = 0.20345976829074747\n",
      "preprocessing: One Hot Encoding (2) k= 9 metric: manhattan\n",
      " acc = 0.8697552136995061\n",
      " f2-score = 0.20032433463703136\n",
      "preprocessing: One Hot Encoding (2) k= 9 metric: jaccard\n",
      " acc = 0.8683873947489963\n",
      " f2-score = 0.19735170837037977\n",
      "preprocessing: One Hot Encoding (2) k= 10 metric: euclidean\n",
      " acc = 0.872970476427328\n",
      " f2-score = 0.14576781970649896\n",
      "preprocessing: One Hot Encoding (2) k= 10 metric: cosine\n",
      " acc = 0.8725796710128966\n",
      " f2-score = 0.14520036668412784\n",
      "preprocessing: One Hot Encoding (2) k= 10 metric: manhattan\n",
      " acc = 0.872739545955164\n",
      " f2-score = 0.1423090795815982\n",
      "preprocessing: One Hot Encoding (2) k= 10 metric: jaccard\n",
      " acc = 0.8723665044232067\n",
      " f2-score = 0.14729924143342923\n",
      "preprocessing: One Hot Encoding (2) k= 11 metric: euclidean\n",
      " acc = 0.8716914768891889\n",
      " f2-score = 0.1929234710214537\n",
      "preprocessing: One Hot Encoding (2) k= 11 metric: cosine\n",
      " acc = 0.8707144633531104\n",
      " f2-score = 0.19125595675952278\n",
      "preprocessing: One Hot Encoding (2) k= 11 metric: manhattan\n",
      " acc = 0.8716204213592923\n",
      " f2-score = 0.19038461538461537\n",
      "preprocessing: One Hot Encoding (2) k= 11 metric: jaccard\n",
      " acc = 0.8706078800582655\n",
      " f2-score = 0.18900233652338128\n",
      "preprocessing: One Hot Encoding (2) k= 12 metric: euclidean\n",
      " acc = 0.8737520872561907\n",
      " f2-score = 0.14195583596214514\n",
      "preprocessing: One Hot Encoding (2) k= 12 metric: cosine\n",
      " acc = 0.873325754076811\n",
      " f2-score = 0.143546164446486\n",
      "preprocessing: One Hot Encoding (2) k= 12 metric: manhattan\n",
      " acc = 0.8734856290190784\n",
      " f2-score = 0.13769618004145692\n",
      "preprocessing: One Hot Encoding (2) k= 12 metric: jaccard\n",
      " acc = 0.8727573098376381\n",
      " f2-score = 0.13952724885095205\n",
      "preprocessing: One Hot Encoding (2) k= 13 metric: euclidean\n",
      " acc = 0.8732369346644403\n",
      " f2-score = 0.18095238095238092\n",
      "preprocessing: One Hot Encoding (2) k= 13 metric: cosine\n",
      " acc = 0.8724730877180517\n",
      " f2-score = 0.184285070495075\n",
      "preprocessing: One Hot Encoding (2) k= 13 metric: manhattan\n",
      " acc = 0.8728106014850606\n",
      " f2-score = 0.1778158209244485\n",
      "preprocessing: One Hot Encoding (2) k= 13 metric: jaccard\n",
      " acc = 0.8709098660603262\n",
      " f2-score = 0.17615612520932628\n",
      "preprocessing: One Hot Encoding (2) k= 14 metric: euclidean\n",
      " acc = 0.8746225174974243\n",
      " f2-score = 0.14233997038012178\n",
      "preprocessing: One Hot Encoding (2) k= 14 metric: cosine\n",
      " acc = 0.874480406437631\n",
      " f2-score = 0.14245764105938477\n",
      "preprocessing: One Hot Encoding (2) k= 14 metric: manhattan\n",
      " acc = 0.8741961843180446\n",
      " f2-score = 0.13756631190484034\n",
      "preprocessing: One Hot Encoding (2) k= 14 metric: jaccard\n",
      " acc = 0.8729171847799055\n",
      " f2-score = 0.1366192088909348\n",
      "preprocessing: One Hot Encoding (2) k= 15 metric: euclidean\n",
      " acc = 0.8732546985469144\n",
      " f2-score = 0.17389896373056993\n",
      "preprocessing: One Hot Encoding (2) k= 15 metric: cosine\n",
      " acc = 0.8732546985469144\n",
      " f2-score = 0.17735885662549314\n",
      "preprocessing: One Hot Encoding (2) k= 15 metric: manhattan\n",
      " acc = 0.8736810317262941\n",
      " f2-score = 0.17388345109224085\n",
      "preprocessing: One Hot Encoding (2) k= 15 metric: jaccard\n",
      " acc = 0.8718158240665079\n",
      " f2-score = 0.169680696192294\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"One Hot Encoding (2)\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_one_hot_encoded_all_columns_train_test_split()\n",
    "\n",
    "for n_neighbors in range(n_neighbors_start, n_neighbors_end):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding (1) + Oversampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: One Hot Encoding (1) + Oversampling k= 1 metric: euclidean\n",
      " acc = 0.8200518705368245\n",
      " f2-score = 0.3002190975400061\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 1 metric: cosine\n",
      " acc = 0.8217039116069208\n",
      " f2-score = 0.2938797143494756\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 1 metric: manhattan\n",
      " acc = 0.8205847870110491\n",
      " f2-score = 0.30083534537784806\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 1 metric: jaccard\n",
      " acc = 0.8191459125306427\n",
      " f2-score = 0.30210811708072816\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 2 metric: euclidean\n",
      " acc = 0.8204426759512559\n",
      " f2-score = 0.29954748618861216\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 2 metric: cosine\n",
      " acc = 0.8220591892564039\n",
      " f2-score = 0.29367427837641674\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 2 metric: manhattan\n",
      " acc = 0.821028884072903\n",
      " f2-score = 0.3001889203756182\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 2 metric: jaccard\n",
      " acc = 0.8480655131985647\n",
      " f2-score = 0.21035844471445928\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 3 metric: euclidean\n",
      " acc = 0.773492734572068\n",
      " f2-score = 0.4151694668350005\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 3 metric: cosine\n",
      " acc = 0.7768856361246314\n",
      " f2-score = 0.40955714950182376\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 3 metric: manhattan\n",
      " acc = 0.7744875119906207\n",
      " f2-score = 0.4176584891911586\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 3 metric: jaccard\n",
      " acc = 0.8197854122997122\n",
      " f2-score = 0.34326248844920365\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 4 metric: euclidean\n",
      " acc = 0.7745585675205173\n",
      " f2-score = 0.41313662479306645\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 4 metric: cosine\n",
      " acc = 0.7779692329555548\n",
      " f2-score = 0.40781012223258556\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 4 metric: manhattan\n",
      " acc = 0.7755355810565957\n",
      " f2-score = 0.415824464846699\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 4 metric: jaccard\n",
      " acc = 0.7621771414360322\n",
      " f2-score = 0.4485751143426762\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 5 metric: euclidean\n",
      " acc = 0.7362241091412939\n",
      " f2-score = 0.47468213745887783\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 5 metric: cosine\n",
      " acc = 0.740292038227875\n",
      " f2-score = 0.46748378117493883\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 5 metric: manhattan\n",
      " acc = 0.73693466444026\n",
      " f2-score = 0.47492435030259883\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 5 metric: jaccard\n",
      " acc = 0.7162752691228195\n",
      " f2-score = 0.4978576453283877\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 6 metric: euclidean\n",
      " acc = 0.7402565104629267\n",
      " f2-score = 0.46919505685513724\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 6 metric: cosine\n",
      " acc = 0.7443422034319821\n",
      " f2-score = 0.46216393739666145\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 6 metric: manhattan\n",
      " acc = 0.7408071908196255\n",
      " f2-score = 0.46943035417695383\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 6 metric: jaccard\n",
      " acc = 0.7391018581021068\n",
      " f2-score = 0.48332592263228097\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 7 metric: euclidean\n",
      " acc = 0.7089032578960458\n",
      " f2-score = 0.5033641715727503\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 7 metric: cosine\n",
      " acc = 0.7135041034568516\n",
      " f2-score = 0.4957817115413223\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 7 metric: manhattan\n",
      " acc = 0.709507229900167\n",
      " f2-score = 0.5042281963902563\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 7 metric: jaccard\n",
      " acc = 0.7018509965538068\n",
      " f2-score = 0.5128709590573032\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 8 metric: euclidean\n",
      " acc = 0.7203787259743489\n",
      " f2-score = 0.49447834307322097\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 8 metric: cosine\n",
      " acc = 0.7247664049454648\n",
      " f2-score = 0.48737872506674185\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 8 metric: manhattan\n",
      " acc = 0.7210359896258927\n",
      " f2-score = 0.4950441830964718\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 8 metric: jaccard\n",
      " acc = 0.7241269051763953\n",
      " f2-score = 0.5046542124384634\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 9 metric: euclidean\n",
      " acc = 0.6924716666074537\n",
      " f2-score = 0.5119663185431953\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 9 metric: cosine\n",
      " acc = 0.6978541229971222\n",
      " f2-score = 0.507452826293406\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 9 metric: manhattan\n",
      " acc = 0.6925604860198245\n",
      " f2-score = 0.5112869586553797\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 9 metric: jaccard\n",
      " acc = 0.6922407361352897\n",
      " f2-score = 0.5222560975609757\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 10 metric: euclidean\n",
      " acc = 0.7118342985042811\n",
      " f2-score = 0.5067147344675648\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 10 metric: cosine\n",
      " acc = 0.715866699825914\n",
      " f2-score = 0.5000851208716377\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 10 metric: manhattan\n",
      " acc = 0.7117987707393328\n",
      " f2-score = 0.5062730627306273\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 10 metric: jaccard\n",
      " acc = 0.715298255586741\n",
      " f2-score = 0.5192056606157605\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 11 metric: euclidean\n",
      " acc = 0.6880662237538636\n",
      " f2-score = 0.5231325349865709\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 11 metric: cosine\n",
      " acc = 0.6944789853270331\n",
      " f2-score = 0.5179425348428496\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 11 metric: manhattan\n",
      " acc = 0.6890077095249938\n",
      " f2-score = 0.5229736288626436\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 11 metric: jaccard\n",
      " acc = 0.6878530571641738\n",
      " f2-score = 0.5308773552655393\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 12 metric: euclidean\n",
      " acc = 0.7098980353145984\n",
      " f2-score = 0.51623498891167\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 12 metric: cosine\n",
      " acc = 0.7160798664156038\n",
      " f2-score = 0.5118085241461041\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 12 metric: manhattan\n",
      " acc = 0.710892812733151\n",
      " f2-score = 0.5162587998659067\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 12 metric: jaccard\n",
      " acc = 0.7111770348527374\n",
      " f2-score = 0.5288261268016329\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 13 metric: euclidean\n",
      " acc = 0.6895406259992184\n",
      " f2-score = 0.5264859186229747\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 13 metric: cosine\n",
      " acc = 0.6967172345187764\n",
      " f2-score = 0.5245412562834607\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 13 metric: manhattan\n",
      " acc = 0.6894340427043735\n",
      " f2-score = 0.524826136179848\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 13 metric: jaccard\n",
      " acc = 0.6872668490425268\n",
      " f2-score = 0.5364441416893733\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 14 metric: euclidean\n",
      " acc = 0.7121540483888159\n",
      " f2-score = 0.5193553119694843\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 14 metric: cosine\n",
      " acc = 0.7185135183145628\n",
      " f2-score = 0.5164120277171494\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 14 metric: manhattan\n",
      " acc = 0.7127224926279888\n",
      " f2-score = 0.5189560036086691\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 14 metric: jaccard\n",
      " acc = 0.7085302163640885\n",
      " f2-score = 0.5347798208782344\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 15 metric: euclidean\n",
      " acc = 0.6935907912033255\n",
      " f2-score = 0.5298019962671427\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 15 metric: cosine\n",
      " acc = 0.7012292606672115\n",
      " f2-score = 0.5292210457194452\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 15 metric: manhattan\n",
      " acc = 0.6957935126301205\n",
      " f2-score = 0.5319386842319266\n",
      "preprocessing: One Hot Encoding (1) + Oversampling k= 15 metric: jaccard\n",
      " acc = 0.6882616264610794\n",
      " f2-score = 0.5442645883293366\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"One Hot Encoding (1) + Oversampling\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_one_hot_encoded_train_test_split_oversampled()\n",
    "\n",
    "for n_neighbors in range(n_neighbors_start, n_neighbors_end):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding (2) + Oversampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: One Hot Encoding (2) + Oversampling k= 1 metric: euclidean\n",
      " acc = 0.8222190641986713\n",
      " f2-score = 0.3062442735374962\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 1 metric: cosine\n",
      " acc = 0.8196077734749707\n",
      " f2-score = 0.30342022248049144\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 1 metric: manhattan\n",
      " acc = 0.8229651472625857\n",
      " f2-score = 0.30696756621550264\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 1 metric: jaccard\n",
      " acc = 0.8211177034852737\n",
      " f2-score = 0.30364821750589543\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 2 metric: euclidean\n",
      " acc = 0.8227164529079476\n",
      " f2-score = 0.30550690795874685\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 2 metric: cosine\n",
      " acc = 0.820229509361566\n",
      " f2-score = 0.3031058654032638\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 2 metric: manhattan\n",
      " acc = 0.8235513553842327\n",
      " f2-score = 0.3062729600356229\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 2 metric: jaccard\n",
      " acc = 0.8430028066934309\n",
      " f2-score = 0.22974787408623007\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 3 metric: euclidean\n",
      " acc = 0.7763527196504068\n",
      " f2-score = 0.42206608154433395\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 3 metric: cosine\n",
      " acc = 0.7743098731658792\n",
      " f2-score = 0.42229975287105687\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 3 metric: manhattan\n",
      " acc = 0.7765481223576225\n",
      " f2-score = 0.42298716452742124\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 3 metric: jaccard\n",
      " acc = 0.812129178953352\n",
      " f2-score = 0.3613496769390306\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 4 metric: euclidean\n",
      " acc = 0.7774540803638044\n",
      " f2-score = 0.41976542878739853\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 4 metric: cosine\n",
      " acc = 0.7754112338792767\n",
      " f2-score = 0.4211190985039828\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 4 metric: manhattan\n",
      " acc = 0.7775606636586493\n",
      " f2-score = 0.420638209697472\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 4 metric: jaccard\n",
      " acc = 0.766547056524674\n",
      " f2-score = 0.4310693951449603\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 5 metric: euclidean\n",
      " acc = 0.7396525384588055\n",
      " f2-score = 0.4775399995537009\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 5 metric: cosine\n",
      " acc = 0.7368280811454151\n",
      " f2-score = 0.4776504043366213\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 5 metric: manhattan\n",
      " acc = 0.7394926635165382\n",
      " f2-score = 0.47627020269666936\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 5 metric: jaccard\n",
      " acc = 0.7250861548299996\n",
      " f2-score = 0.4860824406249321\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 6 metric: euclidean\n",
      " acc = 0.7456212029701211\n",
      " f2-score = 0.473127017448816\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 6 metric: cosine\n",
      " acc = 0.7430099122464205\n",
      " f2-score = 0.47375044947860484\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 6 metric: manhattan\n",
      " acc = 0.7454613280278538\n",
      " f2-score = 0.472024024566474\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 6 metric: jaccard\n",
      " acc = 0.7428678011866273\n",
      " f2-score = 0.4735742213833087\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 7 metric: euclidean\n",
      " acc = 0.7132198813372651\n",
      " f2-score = 0.5035019784591296\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 7 metric: cosine\n",
      " acc = 0.7095605215475894\n",
      " f2-score = 0.5008113291045877\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 7 metric: manhattan\n",
      " acc = 0.7116744235620137\n",
      " f2-score = 0.5019015423621381\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 7 metric: jaccard\n",
      " acc = 0.7101467296692365\n",
      " f2-score = 0.5083837619768028\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 8 metric: euclidean\n",
      " acc = 0.7234341137599034\n",
      " f2-score = 0.49570732128219513\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 8 metric: cosine\n",
      " acc = 0.7196504067929087\n",
      " f2-score = 0.49223298884902134\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 8 metric: manhattan\n",
      " acc = 0.7218176004547554\n",
      " f2-score = 0.49361307972341295\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 8 metric: jaccard\n",
      " acc = 0.7284790563825629\n",
      " f2-score = 0.49650167311285903\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 9 metric: euclidean\n",
      " acc = 0.6939105410878602\n",
      " f2-score = 0.5126525264106134\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 9 metric: cosine\n",
      " acc = 0.6915479447187978\n",
      " f2-score = 0.512558917749801\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 9 metric: manhattan\n",
      " acc = 0.6940348882651792\n",
      " f2-score = 0.5124004177845133\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 9 metric: jaccard\n",
      " acc = 0.6992574697125804\n",
      " f2-score = 0.5167301554617523\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 10 metric: euclidean\n",
      " acc = 0.712047465093971\n",
      " f2-score = 0.5068429598700999\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 10 metric: cosine\n",
      " acc = 0.709738160372331\n",
      " f2-score = 0.5077197773343136\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 10 metric: manhattan\n",
      " acc = 0.7122961594486091\n",
      " f2-score = 0.5063197653562912\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 10 metric: jaccard\n",
      " acc = 0.718993143141365\n",
      " f2-score = 0.5093636944030645\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 11 metric: euclidean\n",
      " acc = 0.6881195154012861\n",
      " f2-score = 0.5215660118037028\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 11 metric: cosine\n",
      " acc = 0.6860766689167584\n",
      " f2-score = 0.5222707951409175\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 11 metric: manhattan\n",
      " acc = 0.6881905709311827\n",
      " f2-score = 0.5187246963562753\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 11 metric: jaccard\n",
      " acc = 0.6949763740363094\n",
      " f2-score = 0.5253453404506744\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 12 metric: euclidean\n",
      " acc = 0.7118165346218069\n",
      " f2-score = 0.5152399345060666\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 12 metric: cosine\n",
      " acc = 0.7099335630795467\n",
      " f2-score = 0.5165885699941427\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 12 metric: manhattan\n",
      " acc = 0.7111770348527374\n",
      " f2-score = 0.5115140563936631\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 12 metric: jaccard\n",
      " acc = 0.7165061995949835\n",
      " f2-score = 0.521619115849804\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 13 metric: euclidean\n",
      " acc = 0.6911571393043664\n",
      " f2-score = 0.5264969726424074\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 13 metric: cosine\n",
      " acc = 0.6896649731765374\n",
      " f2-score = 0.5279948311022271\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 13 metric: manhattan\n",
      " acc = 0.6910860837744698\n",
      " f2-score = 0.523813385778877\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 13 metric: jaccard\n",
      " acc = 0.6956691654528013\n",
      " f2-score = 0.5339411537758645\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 14 metric: euclidean\n",
      " acc = 0.7142857142857143\n",
      " f2-score = 0.5218286050271939\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 14 metric: cosine\n",
      " acc = 0.7124205066259282\n",
      " f2-score = 0.520846427823172\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 14 metric: manhattan\n",
      " acc = 0.713912672753757\n",
      " f2-score = 0.518937323973265\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 14 metric: jaccard\n",
      " acc = 0.7156002415888016\n",
      " f2-score = 0.5296028502567327\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 15 metric: euclidean\n",
      " acc = 0.6960599708672327\n",
      " f2-score = 0.5339401450659298\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 15 metric: cosine\n",
      " acc = 0.693892777205386\n",
      " f2-score = 0.5314230090649145\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 15 metric: manhattan\n",
      " acc = 0.6966817067538281\n",
      " f2-score = 0.5317589576547231\n",
      "preprocessing: One Hot Encoding (2) + Oversampling k= 15 metric: jaccard\n",
      " acc = 0.697250150993001\n",
      " f2-score = 0.5401704199634815\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"One Hot Encoding (2) + Oversampling\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_one_hot_encoded_all_columns_train_test_split_oversampled()\n",
    "\n",
    "for n_neighbors in range(n_neighbors_start, n_neighbors_end):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding (1) + Undersampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: One Hot Encoding (1) + Undersampling k= 1 metric: euclidean\n",
      " acc = 0.6672647173766298\n",
      " f2-score = 0.49051263653033567\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 1 metric: cosine\n",
      " acc = 0.67145699364053\n",
      " f2-score = 0.4897713598074609\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 1 metric: manhattan\n",
      " acc = 0.6680463282054926\n",
      " f2-score = 0.49166434012682964\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 1 metric: jaccard\n",
      " acc = 0.7487121185206239\n",
      " f2-score = 0.41469277263074367\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: euclidean\n",
      " acc = 0.7785376771947277\n",
      " f2-score = 0.4340126152353227\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: cosine\n",
      " acc = 0.7817707038050237\n",
      " f2-score = 0.4296817715207508\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: manhattan\n",
      " acc = 0.7785021494297794\n",
      " f2-score = 0.43499345327578676\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: jaccard\n",
      " acc = 0.8227342167904217\n",
      " f2-score = 0.3224649674500717\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 3 metric: euclidean\n",
      " acc = 0.6887057235229331\n",
      " f2-score = 0.5292481915086544\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 3 metric: cosine\n",
      " acc = 0.6937861939105411\n",
      " f2-score = 0.5272782688204816\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 3 metric: manhattan\n",
      " acc = 0.6888833623476747\n",
      " f2-score = 0.5283216712709968\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 3 metric: jaccard\n",
      " acc = 0.765854265108182\n",
      " f2-score = 0.4526937929735329\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 4 metric: euclidean\n",
      " acc = 0.7556577965680179\n",
      " f2-score = 0.5046958956066284\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 4 metric: cosine\n",
      " acc = 0.7613244750772729\n",
      " f2-score = 0.5009016000547808\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 4 metric: manhattan\n",
      " acc = 0.7563328241020357\n",
      " f2-score = 0.5061204209765573\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 4 metric: jaccard\n",
      " acc = 0.7441290368422923\n",
      " f2-score = 0.5168234801577682\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 5 metric: euclidean\n",
      " acc = 0.6963086652218708\n",
      " f2-score = 0.5473465350531904\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 5 metric: cosine\n",
      " acc = 0.7028102462074111\n",
      " f2-score = 0.5467332585872999\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 5 metric: manhattan\n",
      " acc = 0.696894873343518\n",
      " f2-score = 0.5471591368301949\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 5 metric: jaccard\n",
      " acc = 0.6802856432301844\n",
      " f2-score = 0.5470060175594357\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 6 metric: euclidean\n",
      " acc = 0.744093509077344\n",
      " f2-score = 0.5324939340284609\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 6 metric: cosine\n",
      " acc = 0.7500977013536079\n",
      " f2-score = 0.5293402585921096\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 6 metric: manhattan\n",
      " acc = 0.7446619533165169\n",
      " f2-score = 0.5345863840017484\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 6 metric: jaccard\n",
      " acc = 0.7302199168650301\n",
      " f2-score = 0.5388142461079122\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 7 metric: euclidean\n",
      " acc = 0.6993995807723736\n",
      " f2-score = 0.555735023925377\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 7 metric: cosine\n",
      " acc = 0.7055281202259566\n",
      " f2-score = 0.5540149259818116\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 7 metric: manhattan\n",
      " acc = 0.7005897608981418\n",
      " f2-score = 0.5578228543720705\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 7 metric: jaccard\n",
      " acc = 0.6807475041745124\n",
      " f2-score = 0.5581728598629841\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 8 metric: euclidean\n",
      " acc = 0.7375741642093296\n",
      " f2-score = 0.5479819665092314\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 8 metric: cosine\n",
      " acc = 0.7453547447330089\n",
      " f2-score = 0.5464944328462075\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 8 metric: manhattan\n",
      " acc = 0.7380182612711834\n",
      " f2-score = 0.5478569126651628\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 8 metric: jaccard\n",
      " acc = 0.7203609620918748\n",
      " f2-score = 0.552623344111818\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 9 metric: euclidean\n",
      " acc = 0.7019931076136\n",
      " f2-score = 0.5650577865628593\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 9 metric: cosine\n",
      " acc = 0.7079972998898639\n",
      " f2-score = 0.5609029916185737\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 9 metric: manhattan\n",
      " acc = 0.703272107151739\n",
      " f2-score = 0.5655669811511344\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 9 metric: jaccard\n",
      " acc = 0.681511351120901\n",
      " f2-score = 0.5645761516765904\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 10 metric: euclidean\n",
      " acc = 0.7333641240629551\n",
      " f2-score = 0.5604535339620642\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 10 metric: cosine\n",
      " acc = 0.7398834689309696\n",
      " f2-score = 0.551696701217152\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 10 metric: manhattan\n",
      " acc = 0.7353181511351121\n",
      " f2-score = 0.560444699530217\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 10 metric: jaccard\n",
      " acc = 0.7144100614630333\n",
      " f2-score = 0.5605205716403773\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 11 metric: euclidean\n",
      " acc = 0.7027391906775144\n",
      " f2-score = 0.5693563009972801\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 11 metric: cosine\n",
      " acc = 0.7104664795537713\n",
      " f2-score = 0.5643974992849262\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 11 metric: manhattan\n",
      " acc = 0.7048353288094646\n",
      " f2-score = 0.5707130884342752\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 11 metric: jaccard\n",
      " acc = 0.6821863786549188\n",
      " f2-score = 0.5676479799561541\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 12 metric: euclidean\n",
      " acc = 0.7299001669804952\n",
      " f2-score = 0.5650893796004206\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 12 metric: cosine\n",
      " acc = 0.7387110526876755\n",
      " f2-score = 0.5623158703727424\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 12 metric: manhattan\n",
      " acc = 0.7323338188794543\n",
      " f2-score = 0.5676188468430484\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 12 metric: jaccard\n",
      " acc = 0.7102888407290298\n",
      " f2-score = 0.565862519389338\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 13 metric: euclidean\n",
      " acc = 0.7046754538671972\n",
      " f2-score = 0.5739538821252923\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 13 metric: cosine\n",
      " acc = 0.7118698262692295\n",
      " f2-score = 0.5699703991017658\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 13 metric: manhattan\n",
      " acc = 0.7052616619888443\n",
      " f2-score = 0.5734032892745434\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 13 metric: jaccard\n",
      " acc = 0.683589725370377\n",
      " f2-score = 0.5714761093820345\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 14 metric: euclidean\n",
      " acc = 0.7288520979145202\n",
      " f2-score = 0.571921749136939\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 14 metric: cosine\n",
      " acc = 0.7349806373681032\n",
      " f2-score = 0.5654799745708837\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 14 metric: manhattan\n",
      " acc = 0.7297758198031762\n",
      " f2-score = 0.5727061386055667\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 14 metric: jaccard\n",
      " acc = 0.7069847585888371\n",
      " f2-score = 0.5688270603504217\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 15 metric: euclidean\n",
      " acc = 0.7047998010445163\n",
      " f2-score = 0.5766714311597849\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 15 metric: cosine\n",
      " acc = 0.7132554091022134\n",
      " f2-score = 0.5748158426347256\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 15 metric: manhattan\n",
      " acc = 0.7061853838775003\n",
      " f2-score = 0.5771906826661289\n",
      "preprocessing: One Hot Encoding (1) + Undersampling k= 15 metric: jaccard\n",
      " acc = 0.6828258784239883\n",
      " f2-score = 0.5722773825345946\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"One Hot Encoding (1) + Undersampling\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_one_hot_encoded_train_test_split_undersampled()\n",
    "\n",
    "for n_neighbors in range(n_neighbors_start, n_neighbors_end):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding (2) + Undersampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: One Hot Encoding (2) + Undersampling k= 1 metric: euclidean\n",
      " acc = 0.6663765232529222\n",
      " f2-score = 0.4872991187143597\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 1 metric: cosine\n",
      " acc = 0.6646356627704552\n",
      " f2-score = 0.48634880203772984\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 1 metric: manhattan\n",
      " acc = 0.6671581340817849\n",
      " f2-score = 0.4887705441199936\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 1 metric: jaccard\n",
      " acc = 0.7332575407681102\n",
      " f2-score = 0.4474716896513061\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 2 metric: euclidean\n",
      " acc = 0.7783245106050378\n",
      " f2-score = 0.4331845310301325\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 2 metric: cosine\n",
      " acc = 0.7749316090524745\n",
      " f2-score = 0.43008674221373855\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 2 metric: manhattan\n",
      " acc = 0.7789817742565815\n",
      " f2-score = 0.4318662356915449\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 2 metric: jaccard\n",
      " acc = 0.8166412051017871\n",
      " f2-score = 0.34755752164648124\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 3 metric: euclidean\n",
      " acc = 0.6909795004796249\n",
      " f2-score = 0.5334772346734986\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 3 metric: cosine\n",
      " acc = 0.6886524318755107\n",
      " f2-score = 0.533969849246231\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 3 metric: manhattan\n",
      " acc = 0.6908196255373574\n",
      " f2-score = 0.534094116698254\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 3 metric: jaccard\n",
      " acc = 0.7571499626958468\n",
      " f2-score = 0.47895750240682167\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 4 metric: euclidean\n",
      " acc = 0.7551604078587416\n",
      " f2-score = 0.5014009399855387\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 4 metric: cosine\n",
      " acc = 0.7521405478381356\n",
      " f2-score = 0.5049609410074526\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 4 metric: manhattan\n",
      " acc = 0.7556755604504921\n",
      " f2-score = 0.502091104329151\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 4 metric: jaccard\n",
      " acc = 0.7532596724340072\n",
      " f2-score = 0.5082729048335316\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 5 metric: euclidean\n",
      " acc = 0.6994173446548477\n",
      " f2-score = 0.5517339052320641\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 5 metric: cosine\n",
      " acc = 0.6965040679290866\n",
      " f2-score = 0.5540687379265936\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 5 metric: manhattan\n",
      " acc = 0.6994884001847443\n",
      " f2-score = 0.5510687393733301\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 5 metric: jaccard\n",
      " acc = 0.6956691654528013\n",
      " f2-score = 0.5533104127711763\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 6 metric: euclidean\n",
      " acc = 0.7476462855721746\n",
      " f2-score = 0.5367287403215547\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 6 metric: cosine\n",
      " acc = 0.7433651898959036\n",
      " f2-score = 0.5388700269870289\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 6 metric: manhattan\n",
      " acc = 0.7477528688670195\n",
      " f2-score = 0.5363686231486561\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 6 metric: jaccard\n",
      " acc = 0.7443777311969304\n",
      " f2-score = 0.5395393432263407\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 7 metric: euclidean\n",
      " acc = 0.703733968096067\n",
      " f2-score = 0.5607684980645684\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 7 metric: cosine\n",
      " acc = 0.7004121220734003\n",
      " f2-score = 0.5642790997302412\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 7 metric: manhattan\n",
      " acc = 0.7037872597434895\n",
      " f2-score = 0.5606453705205123\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 7 metric: jaccard\n",
      " acc = 0.7002700110136071\n",
      " f2-score = 0.5646556928401819\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 8 metric: euclidean\n",
      " acc = 0.7397591217536504\n",
      " f2-score = 0.5489288405641081\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 8 metric: cosine\n",
      " acc = 0.7367747894979927\n",
      " f2-score = 0.5517152823849165\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 8 metric: manhattan\n",
      " acc = 0.7405940242299357\n",
      " f2-score = 0.5474649406688243\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 8 metric: jaccard\n",
      " acc = 0.737503108679433\n",
      " f2-score = 0.5535339697761034\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 9 metric: euclidean\n",
      " acc = 0.7051906064589477\n",
      " f2-score = 0.5675079438968609\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 9 metric: cosine\n",
      " acc = 0.7014957189043237\n",
      " f2-score = 0.5688652867459216\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 9 metric: manhattan\n",
      " acc = 0.704995203751732\n",
      " f2-score = 0.5653459730999838\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 9 metric: jaccard\n",
      " acc = 0.7029523572672043\n",
      " f2-score = 0.5715148465022648\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 10 metric: euclidean\n",
      " acc = 0.7361352897289232\n",
      " f2-score = 0.5600025546590595\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 10 metric: cosine\n",
      " acc = 0.7331509574732653\n",
      " f2-score = 0.562855752343469\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 10 metric: manhattan\n",
      " acc = 0.7357267204320176\n",
      " f2-score = 0.5579929292499041\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 10 metric: jaccard\n",
      " acc = 0.733914804419654\n",
      " f2-score = 0.564679986454453\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 11 metric: euclidean\n",
      " acc = 0.7062209116424486\n",
      " f2-score = 0.5726271100778327\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 11 metric: cosine\n",
      " acc = 0.7025615518527729\n",
      " f2-score = 0.5735876359105251\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 11 metric: manhattan\n",
      " acc = 0.7068248836465698\n",
      " f2-score = 0.572006881894545\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 11 metric: jaccard\n",
      " acc = 0.7045688705723523\n",
      " f2-score = 0.5759009462452185\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 12 metric: euclidean\n",
      " acc = 0.7325647493516183\n",
      " f2-score = 0.5645910780669146\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 12 metric: cosine\n",
      " acc = 0.7286389313248304\n",
      " f2-score = 0.5675743663920508\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 12 metric: manhattan\n",
      " acc = 0.7335595267701709\n",
      " f2-score = 0.5663228642685041\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 12 metric: jaccard\n",
      " acc = 0.7301488613351335\n",
      " f2-score = 0.5706134094151212\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 13 metric: euclidean\n",
      " acc = 0.7079972998898639\n",
      " f2-score = 0.5749569794513615\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 13 metric: cosine\n",
      " acc = 0.703378690446584\n",
      " f2-score = 0.5762081784386618\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 13 metric: manhattan\n",
      " acc = 0.708583508011511\n",
      " f2-score = 0.5747173023142708\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 13 metric: jaccard\n",
      " acc = 0.7050307315166803\n",
      " f2-score = 0.5777513887770712\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 14 metric: euclidean\n",
      " acc = 0.7299179308629694\n",
      " f2-score = 0.5682700988934847\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 14 metric: cosine\n",
      " acc = 0.7262585710732937\n",
      " f2-score = 0.5723512865923535\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 14 metric: manhattan\n",
      " acc = 0.7310370554588411\n",
      " f2-score = 0.5701588502269289\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 14 metric: jaccard\n",
      " acc = 0.7271112374320532\n",
      " f2-score = 0.5739686907218644\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: euclidean\n",
      " acc = 0.7094894660176928\n",
      " f2-score = 0.5768724029593595\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: cosine\n",
      " acc = 0.7023483852630831\n",
      " f2-score = 0.5769269339537497\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: manhattan\n",
      " acc = 0.7095249937826411\n",
      " f2-score = 0.5773634613436012\n",
      "preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: jaccard\n",
      " acc = 0.7047287455146197\n",
      " f2-score = 0.5804889808546116\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"One Hot Encoding (2) + Undersampling\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, target_train, target_validation = get_preprocessed_brfss_dataset_one_hot_encoded_all_columns_train_test_split_undersampled()\n",
    "\n",
    "for n_neighbors in range(n_neighbors_start, n_neighbors_end):\n",
    "    for metric in metrics:\n",
    "        knn_estimator = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "        knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "        diabetes_test_prediction = knn_estimator.predict(data_validation)\n",
    "        print(evaluation(target_validation, diabetes_test_prediction, n_neighbors, metric, preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of best approach\n",
    "We use the following loop to print all F2- and accuracy-scores and thereby also analyze which approach performed best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr.0) preprocessing: One Hot Encoding (1) + Undersampling k= 1 metric: euclidean\n",
      "Weighted F2-Score = 0.49051263653033567\n",
      "Accuracy-Score = 0.6672647173766298\n",
      "\n",
      "Nr.1) preprocessing: One Hot Encoding (1) + Undersampling k= 1 metric: cosine\n",
      "Weighted F2-Score = 0.4897713598074609\n",
      "Accuracy-Score = 0.67145699364053\n",
      "\n",
      "Nr.2) preprocessing: One Hot Encoding (1) + Undersampling k= 1 metric: manhattan\n",
      "Weighted F2-Score = 0.49166434012682964\n",
      "Accuracy-Score = 0.6680463282054926\n",
      "\n",
      "Nr.3) preprocessing: One Hot Encoding (1) + Undersampling k= 1 metric: jaccard\n",
      "Weighted F2-Score = 0.41469277263074367\n",
      "Accuracy-Score = 0.7487121185206239\n",
      "\n",
      "Nr.4) preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: euclidean\n",
      "Weighted F2-Score = 0.4340126152353227\n",
      "Accuracy-Score = 0.7785376771947277\n",
      "\n",
      "Nr.5) preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: cosine\n",
      "Weighted F2-Score = 0.4296817715207508\n",
      "Accuracy-Score = 0.7817707038050237\n",
      "\n",
      "Nr.6) preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: manhattan\n",
      "Weighted F2-Score = 0.43499345327578676\n",
      "Accuracy-Score = 0.7785021494297794\n",
      "\n",
      "Nr.7) preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: jaccard\n",
      "Weighted F2-Score = 0.3224649674500717\n",
      "Accuracy-Score = 0.8227342167904217\n",
      "\n",
      "Nr.8) preprocessing: One Hot Encoding (1) + Undersampling k= 3 metric: euclidean\n",
      "Weighted F2-Score = 0.5292481915086544\n",
      "Accuracy-Score = 0.6887057235229331\n",
      "\n",
      "Nr.9) preprocessing: One Hot Encoding (1) + Undersampling k= 3 metric: cosine\n",
      "Weighted F2-Score = 0.5272782688204816\n",
      "Accuracy-Score = 0.6937861939105411\n",
      "\n",
      "Nr.10) preprocessing: One Hot Encoding (1) + Undersampling k= 3 metric: manhattan\n",
      "Weighted F2-Score = 0.5283216712709968\n",
      "Accuracy-Score = 0.6888833623476747\n",
      "\n",
      "Nr.11) preprocessing: One Hot Encoding (1) + Undersampling k= 3 metric: jaccard\n",
      "Weighted F2-Score = 0.4526937929735329\n",
      "Accuracy-Score = 0.765854265108182\n",
      "\n",
      "Nr.12) preprocessing: One Hot Encoding (1) + Undersampling k= 4 metric: euclidean\n",
      "Weighted F2-Score = 0.5046958956066284\n",
      "Accuracy-Score = 0.7556577965680179\n",
      "\n",
      "Nr.13) preprocessing: One Hot Encoding (1) + Undersampling k= 4 metric: cosine\n",
      "Weighted F2-Score = 0.5009016000547808\n",
      "Accuracy-Score = 0.7613244750772729\n",
      "\n",
      "Nr.14) preprocessing: One Hot Encoding (1) + Undersampling k= 4 metric: manhattan\n",
      "Weighted F2-Score = 0.5061204209765573\n",
      "Accuracy-Score = 0.7563328241020357\n",
      "\n",
      "Nr.15) preprocessing: One Hot Encoding (1) + Undersampling k= 4 metric: jaccard\n",
      "Weighted F2-Score = 0.5168234801577682\n",
      "Accuracy-Score = 0.7441290368422923\n",
      "\n",
      "Nr.16) preprocessing: One Hot Encoding (1) + Undersampling k= 5 metric: euclidean\n",
      "Weighted F2-Score = 0.5473465350531904\n",
      "Accuracy-Score = 0.6963086652218708\n",
      "\n",
      "Nr.17) preprocessing: One Hot Encoding (1) + Undersampling k= 5 metric: cosine\n",
      "Weighted F2-Score = 0.5467332585872999\n",
      "Accuracy-Score = 0.7028102462074111\n",
      "\n",
      "Nr.18) preprocessing: One Hot Encoding (1) + Undersampling k= 5 metric: manhattan\n",
      "Weighted F2-Score = 0.5471591368301949\n",
      "Accuracy-Score = 0.696894873343518\n",
      "\n",
      "Nr.19) preprocessing: One Hot Encoding (1) + Undersampling k= 5 metric: jaccard\n",
      "Weighted F2-Score = 0.5470060175594357\n",
      "Accuracy-Score = 0.6802856432301844\n",
      "\n",
      "Nr.20) preprocessing: One Hot Encoding (1) + Undersampling k= 6 metric: euclidean\n",
      "Weighted F2-Score = 0.5324939340284609\n",
      "Accuracy-Score = 0.744093509077344\n",
      "\n",
      "Nr.21) preprocessing: One Hot Encoding (1) + Undersampling k= 6 metric: cosine\n",
      "Weighted F2-Score = 0.5293402585921096\n",
      "Accuracy-Score = 0.7500977013536079\n",
      "\n",
      "Nr.22) preprocessing: One Hot Encoding (1) + Undersampling k= 6 metric: manhattan\n",
      "Weighted F2-Score = 0.5345863840017484\n",
      "Accuracy-Score = 0.7446619533165169\n",
      "\n",
      "Nr.23) preprocessing: One Hot Encoding (1) + Undersampling k= 6 metric: jaccard\n",
      "Weighted F2-Score = 0.5388142461079122\n",
      "Accuracy-Score = 0.7302199168650301\n",
      "\n",
      "Nr.24) preprocessing: One Hot Encoding (1) + Undersampling k= 7 metric: euclidean\n",
      "Weighted F2-Score = 0.555735023925377\n",
      "Accuracy-Score = 0.6993995807723736\n",
      "\n",
      "Nr.25) preprocessing: One Hot Encoding (1) + Undersampling k= 7 metric: cosine\n",
      "Weighted F2-Score = 0.5540149259818116\n",
      "Accuracy-Score = 0.7055281202259566\n",
      "\n",
      "Nr.26) preprocessing: One Hot Encoding (1) + Undersampling k= 7 metric: manhattan\n",
      "Weighted F2-Score = 0.5578228543720705\n",
      "Accuracy-Score = 0.7005897608981418\n",
      "\n",
      "Nr.27) preprocessing: One Hot Encoding (1) + Undersampling k= 7 metric: jaccard\n",
      "Weighted F2-Score = 0.5581728598629841\n",
      "Accuracy-Score = 0.6807475041745124\n",
      "\n",
      "Nr.28) preprocessing: One Hot Encoding (1) + Undersampling k= 8 metric: euclidean\n",
      "Weighted F2-Score = 0.5479819665092314\n",
      "Accuracy-Score = 0.7375741642093296\n",
      "\n",
      "Nr.29) preprocessing: One Hot Encoding (1) + Undersampling k= 8 metric: cosine\n",
      "Weighted F2-Score = 0.5464944328462075\n",
      "Accuracy-Score = 0.7453547447330089\n",
      "\n",
      "Nr.30) preprocessing: One Hot Encoding (1) + Undersampling k= 8 metric: manhattan\n",
      "Weighted F2-Score = 0.5478569126651628\n",
      "Accuracy-Score = 0.7380182612711834\n",
      "\n",
      "Nr.31) preprocessing: One Hot Encoding (1) + Undersampling k= 8 metric: jaccard\n",
      "Weighted F2-Score = 0.552623344111818\n",
      "Accuracy-Score = 0.7203609620918748\n",
      "\n",
      "Nr.32) preprocessing: One Hot Encoding (1) + Undersampling k= 9 metric: euclidean\n",
      "Weighted F2-Score = 0.5650577865628593\n",
      "Accuracy-Score = 0.7019931076136\n",
      "\n",
      "Nr.33) preprocessing: One Hot Encoding (1) + Undersampling k= 9 metric: cosine\n",
      "Weighted F2-Score = 0.5609029916185737\n",
      "Accuracy-Score = 0.7079972998898639\n",
      "\n",
      "Nr.34) preprocessing: One Hot Encoding (1) + Undersampling k= 9 metric: manhattan\n",
      "Weighted F2-Score = 0.5655669811511344\n",
      "Accuracy-Score = 0.703272107151739\n",
      "\n",
      "Nr.35) preprocessing: One Hot Encoding (1) + Undersampling k= 9 metric: jaccard\n",
      "Weighted F2-Score = 0.5645761516765904\n",
      "Accuracy-Score = 0.681511351120901\n",
      "\n",
      "Nr.36) preprocessing: One Hot Encoding (1) + Undersampling k= 10 metric: euclidean\n",
      "Weighted F2-Score = 0.5604535339620642\n",
      "Accuracy-Score = 0.7333641240629551\n",
      "\n",
      "Nr.37) preprocessing: One Hot Encoding (1) + Undersampling k= 10 metric: cosine\n",
      "Weighted F2-Score = 0.551696701217152\n",
      "Accuracy-Score = 0.7398834689309696\n",
      "\n",
      "Nr.38) preprocessing: One Hot Encoding (1) + Undersampling k= 10 metric: manhattan\n",
      "Weighted F2-Score = 0.560444699530217\n",
      "Accuracy-Score = 0.7353181511351121\n",
      "\n",
      "Nr.39) preprocessing: One Hot Encoding (1) + Undersampling k= 10 metric: jaccard\n",
      "Weighted F2-Score = 0.5605205716403773\n",
      "Accuracy-Score = 0.7144100614630333\n",
      "\n",
      "Nr.40) preprocessing: One Hot Encoding (1) + Undersampling k= 11 metric: euclidean\n",
      "Weighted F2-Score = 0.5693563009972801\n",
      "Accuracy-Score = 0.7027391906775144\n",
      "\n",
      "Nr.41) preprocessing: One Hot Encoding (1) + Undersampling k= 11 metric: cosine\n",
      "Weighted F2-Score = 0.5643974992849262\n",
      "Accuracy-Score = 0.7104664795537713\n",
      "\n",
      "Nr.42) preprocessing: One Hot Encoding (1) + Undersampling k= 11 metric: manhattan\n",
      "Weighted F2-Score = 0.5707130884342752\n",
      "Accuracy-Score = 0.7048353288094646\n",
      "\n",
      "Nr.43) preprocessing: One Hot Encoding (1) + Undersampling k= 11 metric: jaccard\n",
      "Weighted F2-Score = 0.5676479799561541\n",
      "Accuracy-Score = 0.6821863786549188\n",
      "\n",
      "Nr.44) preprocessing: One Hot Encoding (1) + Undersampling k= 12 metric: euclidean\n",
      "Weighted F2-Score = 0.5650893796004206\n",
      "Accuracy-Score = 0.7299001669804952\n",
      "\n",
      "Nr.45) preprocessing: One Hot Encoding (1) + Undersampling k= 12 metric: cosine\n",
      "Weighted F2-Score = 0.5623158703727424\n",
      "Accuracy-Score = 0.7387110526876755\n",
      "\n",
      "Nr.46) preprocessing: One Hot Encoding (1) + Undersampling k= 12 metric: manhattan\n",
      "Weighted F2-Score = 0.5676188468430484\n",
      "Accuracy-Score = 0.7323338188794543\n",
      "\n",
      "Nr.47) preprocessing: One Hot Encoding (1) + Undersampling k= 12 metric: jaccard\n",
      "Weighted F2-Score = 0.565862519389338\n",
      "Accuracy-Score = 0.7102888407290298\n",
      "\n",
      "Nr.48) preprocessing: One Hot Encoding (1) + Undersampling k= 13 metric: euclidean\n",
      "Weighted F2-Score = 0.5739538821252923\n",
      "Accuracy-Score = 0.7046754538671972\n",
      "\n",
      "Nr.49) preprocessing: One Hot Encoding (1) + Undersampling k= 13 metric: cosine\n",
      "Weighted F2-Score = 0.5699703991017658\n",
      "Accuracy-Score = 0.7118698262692295\n",
      "\n",
      "Nr.50) preprocessing: One Hot Encoding (1) + Undersampling k= 13 metric: manhattan\n",
      "Weighted F2-Score = 0.5734032892745434\n",
      "Accuracy-Score = 0.7052616619888443\n",
      "\n",
      "Nr.51) preprocessing: One Hot Encoding (1) + Undersampling k= 13 metric: jaccard\n",
      "Weighted F2-Score = 0.5714761093820345\n",
      "Accuracy-Score = 0.683589725370377\n",
      "\n",
      "Nr.52) preprocessing: One Hot Encoding (1) + Undersampling k= 14 metric: euclidean\n",
      "Weighted F2-Score = 0.571921749136939\n",
      "Accuracy-Score = 0.7288520979145202\n",
      "\n",
      "Nr.53) preprocessing: One Hot Encoding (1) + Undersampling k= 14 metric: cosine\n",
      "Weighted F2-Score = 0.5654799745708837\n",
      "Accuracy-Score = 0.7349806373681032\n",
      "\n",
      "Nr.54) preprocessing: One Hot Encoding (1) + Undersampling k= 14 metric: manhattan\n",
      "Weighted F2-Score = 0.5727061386055667\n",
      "Accuracy-Score = 0.7297758198031762\n",
      "\n",
      "Nr.55) preprocessing: One Hot Encoding (1) + Undersampling k= 14 metric: jaccard\n",
      "Weighted F2-Score = 0.5688270603504217\n",
      "Accuracy-Score = 0.7069847585888371\n",
      "\n",
      "Nr.56) preprocessing: One Hot Encoding (1) + Undersampling k= 15 metric: euclidean\n",
      "Weighted F2-Score = 0.5766714311597849\n",
      "Accuracy-Score = 0.7047998010445163\n",
      "\n",
      "Nr.57) preprocessing: One Hot Encoding (1) + Undersampling k= 15 metric: cosine\n",
      "Weighted F2-Score = 0.5748158426347256\n",
      "Accuracy-Score = 0.7132554091022134\n",
      "\n",
      "Nr.58) preprocessing: One Hot Encoding (1) + Undersampling k= 15 metric: manhattan\n",
      "Weighted F2-Score = 0.5771906826661289\n",
      "Accuracy-Score = 0.7061853838775003\n",
      "\n",
      "Nr.59) preprocessing: One Hot Encoding (1) + Undersampling k= 15 metric: jaccard\n",
      "Weighted F2-Score = 0.5722773825345946\n",
      "Accuracy-Score = 0.6828258784239883\n",
      "\n",
      "Nr.60) preprocessing: One Hot Encoding (2) + Undersampling k= 1 metric: euclidean\n",
      "Weighted F2-Score = 0.4872991187143597\n",
      "Accuracy-Score = 0.6663765232529222\n",
      "\n",
      "Nr.61) preprocessing: One Hot Encoding (2) + Undersampling k= 1 metric: cosine\n",
      "Weighted F2-Score = 0.48634880203772984\n",
      "Accuracy-Score = 0.6646356627704552\n",
      "\n",
      "Nr.62) preprocessing: One Hot Encoding (2) + Undersampling k= 1 metric: manhattan\n",
      "Weighted F2-Score = 0.4887705441199936\n",
      "Accuracy-Score = 0.6671581340817849\n",
      "\n",
      "Nr.63) preprocessing: One Hot Encoding (2) + Undersampling k= 1 metric: jaccard\n",
      "Weighted F2-Score = 0.4474716896513061\n",
      "Accuracy-Score = 0.7332575407681102\n",
      "\n",
      "Nr.64) preprocessing: One Hot Encoding (2) + Undersampling k= 2 metric: euclidean\n",
      "Weighted F2-Score = 0.4331845310301325\n",
      "Accuracy-Score = 0.7783245106050378\n",
      "\n",
      "Nr.65) preprocessing: One Hot Encoding (2) + Undersampling k= 2 metric: cosine\n",
      "Weighted F2-Score = 0.43008674221373855\n",
      "Accuracy-Score = 0.7749316090524745\n",
      "\n",
      "Nr.66) preprocessing: One Hot Encoding (2) + Undersampling k= 2 metric: manhattan\n",
      "Weighted F2-Score = 0.4318662356915449\n",
      "Accuracy-Score = 0.7789817742565815\n",
      "\n",
      "Nr.67) preprocessing: One Hot Encoding (2) + Undersampling k= 2 metric: jaccard\n",
      "Weighted F2-Score = 0.34755752164648124\n",
      "Accuracy-Score = 0.8166412051017871\n",
      "\n",
      "Nr.68) preprocessing: One Hot Encoding (2) + Undersampling k= 3 metric: euclidean\n",
      "Weighted F2-Score = 0.5334772346734986\n",
      "Accuracy-Score = 0.6909795004796249\n",
      "\n",
      "Nr.69) preprocessing: One Hot Encoding (2) + Undersampling k= 3 metric: cosine\n",
      "Weighted F2-Score = 0.533969849246231\n",
      "Accuracy-Score = 0.6886524318755107\n",
      "\n",
      "Nr.70) preprocessing: One Hot Encoding (2) + Undersampling k= 3 metric: manhattan\n",
      "Weighted F2-Score = 0.534094116698254\n",
      "Accuracy-Score = 0.6908196255373574\n",
      "\n",
      "Nr.71) preprocessing: One Hot Encoding (2) + Undersampling k= 3 metric: jaccard\n",
      "Weighted F2-Score = 0.47895750240682167\n",
      "Accuracy-Score = 0.7571499626958468\n",
      "\n",
      "Nr.72) preprocessing: One Hot Encoding (2) + Undersampling k= 4 metric: euclidean\n",
      "Weighted F2-Score = 0.5014009399855387\n",
      "Accuracy-Score = 0.7551604078587416\n",
      "\n",
      "Nr.73) preprocessing: One Hot Encoding (2) + Undersampling k= 4 metric: cosine\n",
      "Weighted F2-Score = 0.5049609410074526\n",
      "Accuracy-Score = 0.7521405478381356\n",
      "\n",
      "Nr.74) preprocessing: One Hot Encoding (2) + Undersampling k= 4 metric: manhattan\n",
      "Weighted F2-Score = 0.502091104329151\n",
      "Accuracy-Score = 0.7556755604504921\n",
      "\n",
      "Nr.75) preprocessing: One Hot Encoding (2) + Undersampling k= 4 metric: jaccard\n",
      "Weighted F2-Score = 0.5082729048335316\n",
      "Accuracy-Score = 0.7532596724340072\n",
      "\n",
      "Nr.76) preprocessing: One Hot Encoding (2) + Undersampling k= 5 metric: euclidean\n",
      "Weighted F2-Score = 0.5517339052320641\n",
      "Accuracy-Score = 0.6994173446548477\n",
      "\n",
      "Nr.77) preprocessing: One Hot Encoding (2) + Undersampling k= 5 metric: cosine\n",
      "Weighted F2-Score = 0.5540687379265936\n",
      "Accuracy-Score = 0.6965040679290866\n",
      "\n",
      "Nr.78) preprocessing: One Hot Encoding (2) + Undersampling k= 5 metric: manhattan\n",
      "Weighted F2-Score = 0.5510687393733301\n",
      "Accuracy-Score = 0.6994884001847443\n",
      "\n",
      "Nr.79) preprocessing: One Hot Encoding (2) + Undersampling k= 5 metric: jaccard\n",
      "Weighted F2-Score = 0.5533104127711763\n",
      "Accuracy-Score = 0.6956691654528013\n",
      "\n",
      "Nr.80) preprocessing: One Hot Encoding (2) + Undersampling k= 6 metric: euclidean\n",
      "Weighted F2-Score = 0.5367287403215547\n",
      "Accuracy-Score = 0.7476462855721746\n",
      "\n",
      "Nr.81) preprocessing: One Hot Encoding (2) + Undersampling k= 6 metric: cosine\n",
      "Weighted F2-Score = 0.5388700269870289\n",
      "Accuracy-Score = 0.7433651898959036\n",
      "\n",
      "Nr.82) preprocessing: One Hot Encoding (2) + Undersampling k= 6 metric: manhattan\n",
      "Weighted F2-Score = 0.5363686231486561\n",
      "Accuracy-Score = 0.7477528688670195\n",
      "\n",
      "Nr.83) preprocessing: One Hot Encoding (2) + Undersampling k= 6 metric: jaccard\n",
      "Weighted F2-Score = 0.5395393432263407\n",
      "Accuracy-Score = 0.7443777311969304\n",
      "\n",
      "Nr.84) preprocessing: One Hot Encoding (2) + Undersampling k= 7 metric: euclidean\n",
      "Weighted F2-Score = 0.5607684980645684\n",
      "Accuracy-Score = 0.703733968096067\n",
      "\n",
      "Nr.85) preprocessing: One Hot Encoding (2) + Undersampling k= 7 metric: cosine\n",
      "Weighted F2-Score = 0.5642790997302412\n",
      "Accuracy-Score = 0.7004121220734003\n",
      "\n",
      "Nr.86) preprocessing: One Hot Encoding (2) + Undersampling k= 7 metric: manhattan\n",
      "Weighted F2-Score = 0.5606453705205123\n",
      "Accuracy-Score = 0.7037872597434895\n",
      "\n",
      "Nr.87) preprocessing: One Hot Encoding (2) + Undersampling k= 7 metric: jaccard\n",
      "Weighted F2-Score = 0.5646556928401819\n",
      "Accuracy-Score = 0.7002700110136071\n",
      "\n",
      "Nr.88) preprocessing: One Hot Encoding (2) + Undersampling k= 8 metric: euclidean\n",
      "Weighted F2-Score = 0.5489288405641081\n",
      "Accuracy-Score = 0.7397591217536504\n",
      "\n",
      "Nr.89) preprocessing: One Hot Encoding (2) + Undersampling k= 8 metric: cosine\n",
      "Weighted F2-Score = 0.5517152823849165\n",
      "Accuracy-Score = 0.7367747894979927\n",
      "\n",
      "Nr.90) preprocessing: One Hot Encoding (2) + Undersampling k= 8 metric: manhattan\n",
      "Weighted F2-Score = 0.5474649406688243\n",
      "Accuracy-Score = 0.7405940242299357\n",
      "\n",
      "Nr.91) preprocessing: One Hot Encoding (2) + Undersampling k= 8 metric: jaccard\n",
      "Weighted F2-Score = 0.5535339697761034\n",
      "Accuracy-Score = 0.737503108679433\n",
      "\n",
      "Nr.92) preprocessing: One Hot Encoding (2) + Undersampling k= 9 metric: euclidean\n",
      "Weighted F2-Score = 0.5675079438968609\n",
      "Accuracy-Score = 0.7051906064589477\n",
      "\n",
      "Nr.93) preprocessing: One Hot Encoding (2) + Undersampling k= 9 metric: cosine\n",
      "Weighted F2-Score = 0.5688652867459216\n",
      "Accuracy-Score = 0.7014957189043237\n",
      "\n",
      "Nr.94) preprocessing: One Hot Encoding (2) + Undersampling k= 9 metric: manhattan\n",
      "Weighted F2-Score = 0.5653459730999838\n",
      "Accuracy-Score = 0.704995203751732\n",
      "\n",
      "Nr.95) preprocessing: One Hot Encoding (2) + Undersampling k= 9 metric: jaccard\n",
      "Weighted F2-Score = 0.5715148465022648\n",
      "Accuracy-Score = 0.7029523572672043\n",
      "\n",
      "Nr.96) preprocessing: One Hot Encoding (2) + Undersampling k= 10 metric: euclidean\n",
      "Weighted F2-Score = 0.5600025546590595\n",
      "Accuracy-Score = 0.7361352897289232\n",
      "\n",
      "Nr.97) preprocessing: One Hot Encoding (2) + Undersampling k= 10 metric: cosine\n",
      "Weighted F2-Score = 0.562855752343469\n",
      "Accuracy-Score = 0.7331509574732653\n",
      "\n",
      "Nr.98) preprocessing: One Hot Encoding (2) + Undersampling k= 10 metric: manhattan\n",
      "Weighted F2-Score = 0.5579929292499041\n",
      "Accuracy-Score = 0.7357267204320176\n",
      "\n",
      "Nr.99) preprocessing: One Hot Encoding (2) + Undersampling k= 10 metric: jaccard\n",
      "Weighted F2-Score = 0.564679986454453\n",
      "Accuracy-Score = 0.733914804419654\n",
      "\n",
      "Nr.100) preprocessing: One Hot Encoding (2) + Undersampling k= 11 metric: euclidean\n",
      "Weighted F2-Score = 0.5726271100778327\n",
      "Accuracy-Score = 0.7062209116424486\n",
      "\n",
      "Nr.101) preprocessing: One Hot Encoding (2) + Undersampling k= 11 metric: cosine\n",
      "Weighted F2-Score = 0.5735876359105251\n",
      "Accuracy-Score = 0.7025615518527729\n",
      "\n",
      "Nr.102) preprocessing: One Hot Encoding (2) + Undersampling k= 11 metric: manhattan\n",
      "Weighted F2-Score = 0.572006881894545\n",
      "Accuracy-Score = 0.7068248836465698\n",
      "\n",
      "Nr.103) preprocessing: One Hot Encoding (2) + Undersampling k= 11 metric: jaccard\n",
      "Weighted F2-Score = 0.5759009462452185\n",
      "Accuracy-Score = 0.7045688705723523\n",
      "\n",
      "Nr.104) preprocessing: One Hot Encoding (2) + Undersampling k= 12 metric: euclidean\n",
      "Weighted F2-Score = 0.5645910780669146\n",
      "Accuracy-Score = 0.7325647493516183\n",
      "\n",
      "Nr.105) preprocessing: One Hot Encoding (2) + Undersampling k= 12 metric: cosine\n",
      "Weighted F2-Score = 0.5675743663920508\n",
      "Accuracy-Score = 0.7286389313248304\n",
      "\n",
      "Nr.106) preprocessing: One Hot Encoding (2) + Undersampling k= 12 metric: manhattan\n",
      "Weighted F2-Score = 0.5663228642685041\n",
      "Accuracy-Score = 0.7335595267701709\n",
      "\n",
      "Nr.107) preprocessing: One Hot Encoding (2) + Undersampling k= 12 metric: jaccard\n",
      "Weighted F2-Score = 0.5706134094151212\n",
      "Accuracy-Score = 0.7301488613351335\n",
      "\n",
      "Nr.108) preprocessing: One Hot Encoding (2) + Undersampling k= 13 metric: euclidean\n",
      "Weighted F2-Score = 0.5749569794513615\n",
      "Accuracy-Score = 0.7079972998898639\n",
      "\n",
      "Nr.109) preprocessing: One Hot Encoding (2) + Undersampling k= 13 metric: cosine\n",
      "Weighted F2-Score = 0.5762081784386618\n",
      "Accuracy-Score = 0.703378690446584\n",
      "\n",
      "Nr.110) preprocessing: One Hot Encoding (2) + Undersampling k= 13 metric: manhattan\n",
      "Weighted F2-Score = 0.5747173023142708\n",
      "Accuracy-Score = 0.708583508011511\n",
      "\n",
      "Nr.111) preprocessing: One Hot Encoding (2) + Undersampling k= 13 metric: jaccard\n",
      "Weighted F2-Score = 0.5777513887770712\n",
      "Accuracy-Score = 0.7050307315166803\n",
      "\n",
      "Nr.112) preprocessing: One Hot Encoding (2) + Undersampling k= 14 metric: euclidean\n",
      "Weighted F2-Score = 0.5682700988934847\n",
      "Accuracy-Score = 0.7299179308629694\n",
      "\n",
      "Nr.113) preprocessing: One Hot Encoding (2) + Undersampling k= 14 metric: cosine\n",
      "Weighted F2-Score = 0.5723512865923535\n",
      "Accuracy-Score = 0.7262585710732937\n",
      "\n",
      "Nr.114) preprocessing: One Hot Encoding (2) + Undersampling k= 14 metric: manhattan\n",
      "Weighted F2-Score = 0.5701588502269289\n",
      "Accuracy-Score = 0.7310370554588411\n",
      "\n",
      "Nr.115) preprocessing: One Hot Encoding (2) + Undersampling k= 14 metric: jaccard\n",
      "Weighted F2-Score = 0.5739686907218644\n",
      "Accuracy-Score = 0.7271112374320532\n",
      "\n",
      "Nr.116) preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: euclidean\n",
      "Weighted F2-Score = 0.5768724029593595\n",
      "Accuracy-Score = 0.7094894660176928\n",
      "\n",
      "Nr.117) preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: cosine\n",
      "Weighted F2-Score = 0.5769269339537497\n",
      "Accuracy-Score = 0.7023483852630831\n",
      "\n",
      "Nr.118) preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: manhattan\n",
      "Weighted F2-Score = 0.5773634613436012\n",
      "Accuracy-Score = 0.7095249937826411\n",
      "\n",
      "Nr.119) preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: jaccard\n",
      "Weighted F2-Score = 0.5804889808546116\n",
      "Accuracy-Score = 0.7047287455146197\n",
      "\n",
      "--------- Best Approaches ---------\n",
      "Best Approach regarding F2-Score:\n",
      "Nr.119) preprocessing: One Hot Encoding (2) + Undersampling k= 15 metric: jaccard with f2-score = 0.5804889808546116\n",
      "\n",
      "Best Approach regarding Accuracy:\n",
      "Nr.7) preprocessing: One Hot Encoding (1) + Undersampling k= 2 metric: jaccard with acc = 0.8227342167904217\n"
     ]
    }
   ],
   "source": [
    "highest_acc = [0.0, None, None]\n",
    "highest_f2 = [0.0, None, None]\n",
    "\n",
    "for i in range(0, len(approach_list)):\n",
    "    print(\"Nr.{}) {}\".format(i, approach_list[i]))\n",
    "    print (\"Weighted F2-Score = {}\".format(f2_list[i]))\n",
    "    print (\"Accuracy-Score = {}\\n\".format(acc_list[i]))\n",
    "    if highest_f2[0] < float(f2_list[i]):\n",
    "        highest_f2[0] = f2_list[i]\n",
    "        highest_f2[1] = i\n",
    "        highest_f2[2] = approach_list[i]\n",
    "    if highest_acc[0] < float(acc_list[i]):\n",
    "        highest_acc[0] = acc_list[i]\n",
    "        highest_acc[1] = i\n",
    "\n",
    "        highest_acc[2] = approach_list[i]\n",
    "\n",
    "print(\"--------- Best Approaches ---------\")\n",
    "print(\"Best Approach regarding F2-Score:\\nNr.{}) {} with f2-score = {}\\n\".format(highest_f2[1], highest_f2[2], highest_f2[0]))\n",
    "print(\"Best Approach regarding Accuracy:\\nNr.{}) {} with acc = {}\".format(highest_acc[1], highest_acc[2], highest_acc[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that approach preprocessing: Label Encoding + Undersampling with n_neighbors = 15 and metric = manhattan performed best with F2-score of 0.586\n",
    "We test this approach now finally against the test data that we separated at the beginning:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: Label Encoding + Undersampling k= 15 metric: manhattan\n",
      " acc = 0.7178612665423217\n",
      " f2-score = 0.5876770480427772\n"
     ]
    }
   ],
   "source": [
    "preprocessing = \"Label Encoding + Undersampling\"\n",
    "\n",
    "#load data\n",
    "data_train, data_validation, data_test, target_train, target_validation, target_test = get_preprocessed_brfss_dataset_label_encoded_train_test_split_undersampled(include_test_data=True)\n",
    "\n",
    "knn_estimator = KNeighborsClassifier(n_neighbors=15, metric='manhattan')\n",
    "knn_estimator.fit(data_train, target_train.values.ravel())\n",
    "diabetes_test_prediction = knn_estimator.predict(data_test)\n",
    "print(evaluation(target_test, diabetes_test_prediction, 15, 'manhattan', preprocessing))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
